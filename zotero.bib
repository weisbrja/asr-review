@online{baevskiWav2vec20Framework2020,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020-10-22},
  eprint = {2006.11477},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.11477},
  url = {http://arxiv.org/abs/2006.11477},
  urldate = {2026-01-30},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/weisbrja/kit/zotero/storage/N2CWZQE6/Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learning of Speech Representations.pdf;/home/weisbrja/kit/zotero/storage/UYLISQ7X/2006.html}
}

@online{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1607.06450},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2025-12-31},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/7HSYTT88/Ba et al. - 2016 - Layer Normalization.pdf;/home/weisbrja/kit/zotero/storage/T6JNHDPZ/1607.html}
}

@online{chanListenAttendSpell2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  date = {2015-08-20},
  eprint = {1508.01211},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.01211},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {2025-12-22},
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attentionbased recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-toend CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/JVYIM36P/Chan et al. - 2015 - Listen, Attend and Spell.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423/},
  urldate = {2026-01-04},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {/home/weisbrja/kit/zotero/storage/CQP7ZNQ8/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@article{gravesConnectionistTemporalClassification,
  title = {Connectionist {{Temporal Classiﬁcation}}: {{Labelling Unsegmented Sequence Data}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/3C6TSRR6/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf}
}

@online{gravesSequenceTransductionRecurrent2012,
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2012-11-14},
  eprint = {1211.3711},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1211.3711},
  url = {http://arxiv.org/abs/1211.3711},
  urldate = {2025-12-26},
  abstract = {Many machine learning tasks can be expressed as the transformation---or \textbackslash emph\{transduction\}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \textbackslash emph\{finding\} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,CTC,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/LKLBUD5Z/Graves - 2012 - Sequence Transduction with Recurrent Neural Networks.pdf;/home/weisbrja/kit/zotero/storage/HCMBP3VJ/1211.html}
}

@article{hannunSequenceModelingCTC2017,
  title = {Sequence {{Modeling}} with {{CTC}}},
  author = {Hannun, Awni},
  date = {2017-11-27},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {11},
  pages = {e8},
  issn = {2476-0757},
  doi = {10.23915/distill.00008},
  url = {https://distill.pub/2017/ctc},
  urldate = {2025-12-11},
  abstract = {A visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems.},
  langid = {english},
  keywords = {CTC},
  file = {/home/weisbrja/kit/zotero/storage/BVIJWCUE/ctc.html}
}

@online{hendrycksGaussianErrorLinear2023,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2023-06-06},
  eprint = {1606.08415},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.08415},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2025-12-31},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$xΦ(x)\$, where \$Φ(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/3YZA3GS5/Hendrycks and Gimpel - 2023 - Gaussian Error Linear Units (GELUs).pdf;/home/weisbrja/kit/zotero/storage/5KKTNCD9/1606.html}
}

@article{jegouProductQuantizationNearest2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.57},
  url = {https://ieeexplore.ieee.org/document/5432202},
  urldate = {2026-01-04},
  abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
  keywords = {approximate search.,Electronic mail,Euclidean distance,File systems,High-dimensional indexing,Image databases,image indexing,Indexing,Nearest neighbor searches,Neural networks,Permission,Quantization,Scalability,very large databases,wav2vec},
  file = {/home/weisbrja/kit/zotero/storage/T7ZJHX5Y/Jégou et al. - 2011 - Product Quantization for Nearest Neighbor Search.pdf;/home/weisbrja/kit/zotero/storage/56B3VRGA/5432202.html}
}

@book{jurafskySpeechLanguageProcessing2025,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}, with {{Language Models}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  annotation = {Online manuscript released August 24, 2025},
  file = {/home/weisbrja/kit/zotero/storage/DZWKKNRN/ed3book.pdf}
}

@inproceedings{kahnLibriLightBenchmarkASR2020,
  title = {Libri-{{Light}}: {{A Benchmark}} for {{ASR}} with {{Limited}} or {{No Supervision}}},
  shorttitle = {Libri-{{Light}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kahn, Jacob and Rivière, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazaré, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and Likhomanenko, Tatiana and Synnaeve, Gabriel and Joulin, Armand and Mohamed, Abdelrahman and Dupoux, Emmanuel},
  date = {2020-05},
  eprint = {1912.07875},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {7669--7673},
  doi = {10.1109/ICASSP40776.2020.9052942},
  url = {http://arxiv.org/abs/1912.07875},
  urldate = {2026-01-20},
  abstract = {We introduce a new collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio, which is, to our knowledge, the largest freely-available corpus of speech. The audio has been segmented using voice activity detection and is tagged with SNR, speaker ID and genre descriptions. Additionally, we provide baseline systems and evaluation metrics working under three settings: (1) the zero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER, CER) and (3) the distant supervision setting (WER). Settings (2) and (3) use limited textual resources (10 minutes to 10 hours) aligned with the speech. Setting (3) uses large amounts of unaligned text. They are evaluated on the standard LibriSpeech dev and test sets for comparison with the supervised state-of-the-art.},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/weisbrja/kit/zotero/storage/HLEIB9BR/Kahn et al. - 2020 - Libri-Light A Benchmark for ASR with Limited or No Supervision.pdf;/home/weisbrja/kit/zotero/storage/DH5YNDG9/1912.html}
}

@online{liRecentAdvancesEndtoEnd2022,
  title = {Recent {{Advances}} in {{End-to-End Automatic Speech Recognition}}},
  author = {Li, Jinyu},
  date = {2022-02-02},
  eprint = {2111.01690},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2111.01690},
  url = {http://arxiv.org/abs/2111.01690},
  urldate = {2026-01-31},
  abstract = {Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/weisbrja/kit/zotero/storage/YNUB4TXW/Li - 2022 - Recent Advances in End-to-End Automatic Speech Recognition.pdf;/home/weisbrja/kit/zotero/storage/EWQNK2KC/2111.html}
}

@inproceedings{morrisWERRILMER2004,
  title = {From {{WER}} and {{RIL}} to {{MER}} and {{WIL}}: Improved Evaluation Measures for Connected Speech Recognition},
  shorttitle = {From {{WER}} and {{RIL}} to {{MER}} and {{WIL}}},
  author = {Morris, Andrew and Maier, Viktoria and Green, Phil},
  date = {2004-10-04},
  doi = {10.21437/Interspeech.2004-668},
  file = {/home/weisbrja/kit/zotero/storage/5926C8RD/Morris et al. - 2004 - From WER and RIL to MER and WIL improved evaluation measures for connected speech recognition.pdf}
}

@online{NISTSCLITEScoring,
  title = {{{NIST SCLITE Scoring Package Version}} 1.5},
  url = {https://people.csail.mit.edu/joe/sctk-1.2/doc/sclite.htm},
  urldate = {2025-11-17},
  file = {/home/weisbrja/kit/zotero/storage/DFDKNFVG/sclite.html}
}

@inproceedings{panayotovLibrispeechASRCorpus2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  shorttitle = {Librispeech},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2015-04},
  pages = {5206--5210},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2015.7178964},
  url = {https://ieeexplore.ieee.org/document/7178964},
  urldate = {2026-01-20},
  abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Bioinformatics,Blogs,Corpus,Electronic publishing,Genomics,Information services,LibriVox,Resource description framework,Speech Recognition},
  file = {/home/weisbrja/kit/zotero/storage/NXEHCFUA/Panayotov et al. - 2015 - Librispeech An ASR corpus based on public domain audio books.pdf}
}

@inproceedings{parkSpecAugmentSimpleData2019,
  title = {{{SpecAugment}}: {{A Simple Data Augmentation Method}} for {{Automatic Speech Recognition}}},
  shorttitle = {{{SpecAugment}}},
  booktitle = {Interspeech 2019},
  author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  date = {2019-09-15},
  eprint = {1904.08779},
  eprinttype = {arXiv},
  eprintclass = {eess},
  pages = {2613--2617},
  doi = {10.21437/Interspeech.2019-2680},
  url = {http://arxiv.org/abs/1904.08779},
  urldate = {2026-01-06},
  abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8\% WER on test-other without the use of a language model, and 5.8\% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5\% WER. For Switchboard, we achieve 7.2\%/14.6\% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8\%/14.1\% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3\%/17.3\% WER.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/HNJ4UDS3/Park et al. - 2019 - SpecAugment A Simple Data Augmentation Method for Automatic Speech Recognition.pdf;/home/weisbrja/kit/zotero/storage/AHYSSNC3/1904.html}
}

@inproceedings{pengReproducingWhisperStyleTraining2023,
  title = {Reproducing {{Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data}}},
  booktitle = {2023 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and Zhang, Wangyou and Sudo, Yui and Shakeel, Muhammad and Jung, Jee-Weon and Maiti, Soumi and Watanabe, Shinji},
  date = {2023-12},
  pages = {1--8},
  doi = {10.1109/ASRU57964.2023.10389676},
  url = {https://ieeexplore.ieee.org/document/10389676},
  urldate = {2025-12-26},
  abstract = {Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet},
  eventtitle = {2023 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  keywords = {Benchmark testing,Conferences,Data collection,Data models,Pipelines,Pre-training,Robustness,speech recognition,speech translation,Training,whisper},
  file = {/home/weisbrja/kit/zotero/storage/DP5N9J25/Peng et al. - 2023 - Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data.pdf}
}

@online{prabhavalkarEndtoEndSpeechRecognition2023,
  title = {End-to-{{End Speech Recognition}}: {{A Survey}}},
  shorttitle = {End-to-{{End Speech Recognition}}},
  author = {Prabhavalkar, Rohit and Hori, Takaaki and Sainath, Tara N. and Schlüter, Ralf and Watanabe, Shinji},
  date = {2023-03-03},
  eprint = {2303.03329},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2303.03329},
  url = {http://arxiv.org/abs/2303.03329},
  urldate = {2026-01-17},
  abstract = {In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50\% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domain-specific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/weisbrja/kit/zotero/storage/A8EGV447/Prabhavalkar et al. - 2023 - End-to-End Speech Recognition A Survey.pdf;/home/weisbrja/kit/zotero/storage/BT7KZG4R/2303.html}
}

@inproceedings{radfordRobustSpeechRecognition2023,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  date = {2023-07-03},
  pages = {28492--28518},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/radford23a.html},
  urldate = {2025-12-26},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Whisper},
  file = {/home/weisbrja/kit/zotero/storage/ZVUQARE5/Radford et al. - 2023 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf}
}

@online{schneiderWav2vecUnsupervisedPretraining2019,
  title = {Wav2vec: {{Unsupervised Pre-training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  date = {2019-09-11},
  eprint = {1904.05862},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.05862},
  url = {http://arxiv.org/abs/1904.05862},
  urldate = {2026-01-17},
  abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36\% when only a few hours of transcribed data is available. Our approach achieves 2.43\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/weisbrja/kit/zotero/storage/57QP5Y8Z/Schneider et al. - 2019 - wav2vec Unsupervised Pre-training for Speech Recognition.pdf;/home/weisbrja/kit/zotero/storage/WXZB4DPW/1904.html}
}

@online{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014-12-14},
  eprint = {1409.3215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.3215},
  url = {http://arxiv.org/abs/1409.3215},
  urldate = {2026-01-14},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/TPNS5CDK/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf;/home/weisbrja/kit/zotero/storage/A9KW3WJU/1409.html}
}

@online{synnaeveEndtoendASRSupervised2020,
  title = {End-to-End {{ASR}}: From {{Supervised}} to {{Semi-Supervised Learning}} with {{Modern Architectures}}},
  shorttitle = {End-to-End {{ASR}}},
  author = {Synnaeve, Gabriel and Xu, Qiantong and Kahn, Jacob and Likhomanenko, Tatiana and Grave, Edouard and Pratap, Vineel and Sriram, Anuroop and Liptchinsky, Vitaliy and Collobert, Ronan},
  date = {2020-07-15},
  eprint = {1911.08460},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.08460},
  url = {http://arxiv.org/abs/1911.08460},
  urldate = {2026-01-14},
  abstract = {We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/weisbrja/kit/zotero/storage/BGVBYQDR/Synnaeve et al. - 2020 - End-to-end ASR from Supervised to Semi-Supervised Learning with Modern Architectures.pdf;/home/weisbrja/kit/zotero/storage/H5HE32NP/1911.html}
}

@online{taoriMeasuringRobustnessNatural2020,
  title = {Measuring {{Robustness}} to {{Natural Distribution Shifts}} in {{Image Classification}}},
  author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  date = {2020-09-14},
  eprint = {2007.00644},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.00644},
  url = {http://arxiv.org/abs/2007.00644},
  urldate = {2026-01-20},
  abstract = {We study how robust current ImageNet models are to distribution shifts arising from natural variations in datasets. Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. Informed by an evaluation of 204 ImageNet models in 213 different test conditions, we find that there is often little to no transfer of robustness from current synthetic to natural distribution shift. Moreover, most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger and more diverse datasets, which in multiple cases increases robustness, but is still far from closing the performance gaps. Our results indicate that distribution shifts arising in real data are currently an open research problem. We provide our testbed and data as a resource for future work at https://modestyachts.github.io/imagenet-testbed/ .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/AQ8IWJGA/Taori et al. - 2020 - Measuring Robustness to Natural Distribution Shifts in Image Classification.pdf;/home/weisbrja/kit/zotero/storage/2IXWMGB6/2007.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/HRI243JM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/weisbrja/kit/zotero/storage/6X6N8E8I/1706.html}
}
