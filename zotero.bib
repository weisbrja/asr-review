@article{baevskiWav2vec20Framework,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/79S7DCZ8/Baevski et al. - wav2vec 2.0 A Framework for Self-Supervised Learning of Speech Representations.pdf}
}

@online{chanListenAttendSpell2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  date = {2015-08-20},
  eprint = {1508.01211},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.01211},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {2025-12-22},
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attentionbased recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-toend CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/JVYIM36P/Chan et al. - 2015 - Listen, Attend and Spell.pdf}
}

@article{hannunSequenceModelingCTC2017,
  title = {Sequence {{Modeling}} with {{CTC}}},
  author = {Hannun, Awni},
  date = {2017-11-27},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {11},
  pages = {e8},
  issn = {2476-0757},
  doi = {10.23915/distill.00008},
  url = {https://distill.pub/2017/ctc},
  urldate = {2025-12-11},
  abstract = {A visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems.},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/BVIJWCUE/ctc.html}
}

@book{jurafskySpeechLanguageProcessing2025,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}, with {{Language Models}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  annotation = {Online manuscript released August 24, 2025},
  file = {/home/weisbrja/kit/zotero/storage/DZWKKNRN/ed3book.pdf}
}

@online{NISTSCLITEScoring,
  title = {{{NIST SCLITE Scoring Package Version}} 1.5},
  url = {https://people.csail.mit.edu/joe/sctk-1.2/doc/sclite.htm},
  urldate = {2025-11-17},
  file = {/home/weisbrja/kit/zotero/storage/DFDKNFVG/sclite.html}
}

@article{radfordRobustSpeechRecognition,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/X2DSVBP8/Radford et al. - Robust Speech Recognition via Large-Scale Weak Supervision.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-08},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/YQH6UPAB/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/weisbrja/kit/zotero/storage/6JBV454T/1706.html}
}
