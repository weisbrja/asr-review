@article{baevskiWav2vec20Framework,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/79S7DCZ8/Baevski et al. - wav2vec 2.0 A Framework for Self-Supervised Learning of Speech Representations.pdf}
}

@online{chanListenAttendSpell2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  date = {2015-08-20},
  eprint = {1508.01211},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.01211},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {2025-12-22},
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attentionbased recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-toend CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/JVYIM36P/Chan et al. - 2015 - Listen, Attend and Spell.pdf}
}

@online{gravesSequenceTransductionRecurrent2012,
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2012-11-14},
  eprint = {1211.3711},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1211.3711},
  url = {http://arxiv.org/abs/1211.3711},
  urldate = {2025-12-26},
  abstract = {Many machine learning tasks can be expressed as the transformation---or \textbackslash emph\{transduction\}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \textbackslash emph\{finding\} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,CTC,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/LKLBUD5Z/Graves - 2012 - Sequence Transduction with Recurrent Neural Networks.pdf;/home/weisbrja/kit/zotero/storage/HCMBP3VJ/1211.html}
}

@article{hannunSequenceModelingCTC2017,
  title = {Sequence {{Modeling}} with {{CTC}}},
  author = {Hannun, Awni},
  date = {2017-11-27},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {11},
  pages = {e8},
  issn = {2476-0757},
  doi = {10.23915/distill.00008},
  url = {https://distill.pub/2017/ctc},
  urldate = {2025-12-11},
  abstract = {A visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems.},
  langid = {english},
  keywords = {CTC},
  file = {/home/weisbrja/kit/zotero/storage/BVIJWCUE/ctc.html}
}

@book{jurafskySpeechLanguageProcessing2025,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}, with {{Language Models}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  annotation = {Online manuscript released August 24, 2025},
  file = {/home/weisbrja/kit/zotero/storage/DZWKKNRN/ed3book.pdf}
}

@online{NISTSCLITEScoring,
  title = {{{NIST SCLITE Scoring Package Version}} 1.5},
  url = {https://people.csail.mit.edu/joe/sctk-1.2/doc/sclite.htm},
  urldate = {2025-11-17},
  file = {/home/weisbrja/kit/zotero/storage/DFDKNFVG/sclite.html}
}

@inproceedings{pengReproducingWhisperStyleTraining2023,
  title = {Reproducing {{Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data}}},
  booktitle = {2023 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and Zhang, Wangyou and Sudo, Yui and Shakeel, Muhammad and Jung, Jee-Weon and Maiti, Soumi and Watanabe, Shinji},
  date = {2023-12},
  pages = {1--8},
  doi = {10.1109/ASRU57964.2023.10389676},
  url = {https://ieeexplore.ieee.org/document/10389676},
  urldate = {2025-12-26},
  abstract = {Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet},
  eventtitle = {2023 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  keywords = {Benchmark testing,Conferences,Data collection,Data models,Pipelines,Pre-training,Robustness,speech recognition,speech translation,Training,whisper},
  file = {/home/weisbrja/kit/zotero/storage/DP5N9J25/Peng et al. - 2023 - Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data.pdf}
}

@inproceedings{radfordRobustSpeechRecognition2023,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  date = {2023-07-03},
  pages = {28492--28518},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/radford23a.html},
  urldate = {2025-12-26},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Whisper},
  file = {/home/weisbrja/kit/zotero/storage/ZVUQARE5/Radford et al. - 2023 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/HRI243JM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/weisbrja/kit/zotero/storage/6X6N8E8I/1706.html}
}
