@article{baevskiWav2vec20Framework,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/79S7DCZ8/Baevski et al. - wav2vec 2.0 A Framework for Self-Supervised Learning of Speech Representations.pdf}
}

@online{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1607.06450},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2025-12-31},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/7HSYTT88/Ba et al. - 2016 - Layer Normalization.pdf;/home/weisbrja/kit/zotero/storage/T6JNHDPZ/1607.html}
}

@online{chanListenAttendSpell2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  date = {2015-08-20},
  eprint = {1508.01211},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.01211},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {2025-12-22},
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attentionbased recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-toend CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/JVYIM36P/Chan et al. - 2015 - Listen, Attend and Spell.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423/},
  urldate = {2026-01-04},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {/home/weisbrja/kit/zotero/storage/CQP7ZNQ8/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@article{gravesConnectionistTemporalClassification,
  title = {Connectionist {{Temporal Classiﬁcation}}: {{Labelling Unsegmented Sequence Data}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  langid = {english},
  file = {/home/weisbrja/kit/zotero/storage/3C6TSRR6/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf}
}

@online{gravesSequenceTransductionRecurrent2012,
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2012-11-14},
  eprint = {1211.3711},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1211.3711},
  url = {http://arxiv.org/abs/1211.3711},
  urldate = {2025-12-26},
  abstract = {Many machine learning tasks can be expressed as the transformation---or \textbackslash emph\{transduction\}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \textbackslash emph\{finding\} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,CTC,Statistics - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/LKLBUD5Z/Graves - 2012 - Sequence Transduction with Recurrent Neural Networks.pdf;/home/weisbrja/kit/zotero/storage/HCMBP3VJ/1211.html}
}

@article{hannunSequenceModelingCTC2017,
  title = {Sequence {{Modeling}} with {{CTC}}},
  author = {Hannun, Awni},
  date = {2017-11-27},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {11},
  pages = {e8},
  issn = {2476-0757},
  doi = {10.23915/distill.00008},
  url = {https://distill.pub/2017/ctc},
  urldate = {2025-12-11},
  abstract = {A visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems.},
  langid = {english},
  keywords = {CTC},
  file = {/home/weisbrja/kit/zotero/storage/BVIJWCUE/ctc.html}
}

@online{hendrycksGaussianErrorLinear2023,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2023-06-06},
  eprint = {1606.08415},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.08415},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2025-12-31},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$xΦ(x)\$, where \$Φ(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/3YZA3GS5/Hendrycks and Gimpel - 2023 - Gaussian Error Linear Units (GELUs).pdf;/home/weisbrja/kit/zotero/storage/5KKTNCD9/1606.html}
}

@article{jegouProductQuantizationNearest2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.57},
  url = {https://ieeexplore.ieee.org/document/5432202},
  urldate = {2026-01-04},
  abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
  keywords = {approximate search.,Electronic mail,Euclidean distance,File systems,High-dimensional indexing,Image databases,image indexing,Indexing,Nearest neighbor searches,Neural networks,Permission,Quantization,Scalability,very large databases,wav2vec},
  file = {/home/weisbrja/kit/zotero/storage/T7ZJHX5Y/Jégou et al. - 2011 - Product Quantization for Nearest Neighbor Search.pdf;/home/weisbrja/kit/zotero/storage/56B3VRGA/5432202.html}
}

@book{jurafskySpeechLanguageProcessing2025,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}, with {{Language Models}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  annotation = {Online manuscript released August 24, 2025},
  file = {/home/weisbrja/kit/zotero/storage/DZWKKNRN/ed3book.pdf}
}

@online{NISTSCLITEScoring,
  title = {{{NIST SCLITE Scoring Package Version}} 1.5},
  url = {https://people.csail.mit.edu/joe/sctk-1.2/doc/sclite.htm},
  urldate = {2025-11-17},
  file = {/home/weisbrja/kit/zotero/storage/DFDKNFVG/sclite.html}
}

@inproceedings{pengReproducingWhisperStyleTraining2023,
  title = {Reproducing {{Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data}}},
  booktitle = {2023 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and Zhang, Wangyou and Sudo, Yui and Shakeel, Muhammad and Jung, Jee-Weon and Maiti, Soumi and Watanabe, Shinji},
  date = {2023-12},
  pages = {1--8},
  doi = {10.1109/ASRU57964.2023.10389676},
  url = {https://ieeexplore.ieee.org/document/10389676},
  urldate = {2025-12-26},
  abstract = {Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet},
  eventtitle = {2023 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  keywords = {Benchmark testing,Conferences,Data collection,Data models,Pipelines,Pre-training,Robustness,speech recognition,speech translation,Training,whisper},
  file = {/home/weisbrja/kit/zotero/storage/DP5N9J25/Peng et al. - 2023 - Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data.pdf}
}

@inproceedings{radfordRobustSpeechRecognition2023,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  date = {2023-07-03},
  pages = {28492--28518},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/radford23a.html},
  urldate = {2025-12-26},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Whisper},
  file = {/home/weisbrja/kit/zotero/storage/ZVUQARE5/Radford et al. - 2023 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/weisbrja/kit/zotero/storage/HRI243JM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/weisbrja/kit/zotero/storage/6X6N8E8I/1706.html}
}
