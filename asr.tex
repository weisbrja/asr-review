\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[english]{babel}
\usepackage{siunitx}
\usepackage{mathptmx}

\usepackage{comment}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{csquotes}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\title{A Review of Automatic Speech Recognition}

\author{Jannis Weisbrodt \\
  KIT \\
  \texttt{jannis.weisbrodt@student.kit.edu}
  }

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here.
\end{abstract}

\section{Introduction}

Automatic Speech Recognition (ASR) is the task of transcribing spoken language into writing---one of the earliest goals of computer language processing \cite{jurafskySpeechLanguageProcessing2025}.
Today, automatic transcription is used ubiquitously not only in fields like law, where dictation is essential, but also in voice-controlled assistants as well as automatic captioning for movies, videos, conferences or voice messages.
Advances in ASR not only enhance accessibility for individuals with typing or hearing impairments, but also generally provide a more natural interface for interacting with modern machines.
Despite the progress, accurate automatic transcription of speech by any speaker in any environment still remains a challenging problem that is far from solved.

% TODO: rewrite after comparison rewrite
In this review, we will describe the ASR task and the challenges making it hard to achieve high accuracy in a lot of scenarios.
We will then talk in detail about the different approaches taken by two state-of-the-art ASR systems, comparing their paradigms and performance.
% historic development of performance metric

\section{Challenges in ASR}\label{sec:challenges_in_asr}

The ASR task varies along multiple dimensions, one of them being vocabulary size.
Some low vocabulary tasks in speech recognition such as digit recognition or detecting \enquote{yes} versus \enquote{no} answers have been solved for quite a while.
Tasks that require accurate transcription of speech using an unrestricted large vocabulary, like videos or conversations, represent much harder challenges \cite{jurafskySpeechLanguageProcessing2025}.

A very important dimension is given by the environment and setup used for recording the audio data to be transcribed.
Audio recorded in a quiet studio setting close to a high quality microphone and without much noise is much easier to transcribe than audio recorded on a busy street or inside a restaurant with the source of the audio being far away from the recording microphone.

It also matters who the speaker is talking to.
Conversational speech between humans is much harder to transcribe accurately than speech addressed directly towards a machine, as speakers tend to use a more simple register as well as articulating themselves more clearly in the latter scenarios.
The transcription of conversational speech is also generally much more challenging due to a higher speaker count and---at least for accurate transcription---the added requirement of needing to identify who speaks when (speaker diarization).

At last, varying speaker characteristics, including accent and style, add another dimension to the task of ASR.
Spoken language also frequently contains disfluencies, such as fragmented, repeated or truncated words (e.g. \enquote{main- mainly}), as well as filled pauses (e.g. \enquote{uhm}, \enquote{uh}).
These sources of variation are heavily compounded by the large number of spoken languages, for many of which large amounts of labeled data simply do not exist \cite{baevskiWav2vec20Framework}.
Such low-resource scenarios still remain a hard challenge in ASR \cite{jurafskySpeechLanguageProcessing2025}.
Additionally, code-switching---the use of multiple languages in the same utterance---complicates things even further.

\section{Approaches to ASR}

Historically, there have been many approaches to the ASR task.
In this review, however, we focus on and compare two specific speech recognition models representing distinct paradigms:
wav2vec~2.0 \citep{baevskiWav2vec20Framework}, a self-supervised encoder-only model fine-tuned with CTC loss, and OpenAI's Whisper \citep{radfordRobustSpeechRecognition2023}, a state-of-the-art weakly-supervised encoder-decoder model.

\subsection{Common Techniques in ASR}

Before diving into the mentioned speech recognition models, we will first discuss some common techniques applied in many ASR systems.
These systems often times consist of multiple components, built around a core speech recognition model.
Most of the time, these components are arranged as a pipeline and can include audio format conversion, feature extraction, subsampling, voice activity detection and (inverse) text normalization \cite{radfordRobustSpeechRecognition2023}.

\subsubsection{Speech Feature Extraction}

The most basic representation of audio data, also referred to as the waveform, uses the amplitude of the recorded sound waves at each time frame.
These amplitudes are digitized by sampling and quantizing them at a given sampling frequency (e.g. $\SI{8}{\kilo\hertz}$ for telephone speech or $\SI{16}{\kilo\hertz}$ for microphone data).
Often times, this representation is not used directly as the input to the speech recognition model.
Instead, the system computes the log mel spectrum for small windows of time (characterized by the window size, stride and shape) and uses those feature vectors as the input.
The mel is a unit of pitch representing the human perception of changes in pitch (which is roughly linear for frequencies between $\SI{100}{\hertz}$ and $\SI{1000}{\hertz}$ and logarithmic for frequencies above this range) more accurately than raw frequency values.
By computing the frequencies present in the audio signal for each time window using a Fourier transform, we can sample the mel spectrum values for this time window by accumulating the energy levels of each frequency range using the mel filter bank.
Because human perception of amplitude is also logarithmic, we then take the log of each of the mel spectrum values, resulting in a vector of scalars, each of which is called a channel  \cite{jurafskySpeechLanguageProcessing2025}.
It is also common for ASR systems to then normalize these channels to a zero mean and unit variance.

\subsubsection{Text normalization}

Many ASR systems apply some form of text normalization to the transcriptions in the training data \cite{jurafskySpeechLanguageProcessing2025}.
This step is mostly rule based and might include stripping out punctuation marks, converting everything to upper case or standardizing spelling to the same language variant (e.g. US versus UK English).
If needed, inverse text normalization is then applied to the output of the speech recognition model.

\subsubsection{Attention-based Transformer Architecture}

% TODO: this section is very alone here and should probably be integrated somewhere else
\citet{vaswaniAttentionAllYou2023} showed that the Transformer architecture, which entirely replaces recurrent connections with multi-headed self-attention, achieves performance surpassing competitive recurrent models of its time while requiring substantially lower training cost.
This architecture also generalizes well to different sequence-to-sequence modeling tasks, such as ASR and has since provided the prevalent building blocks of state-of-the-art ASR systems.

\subsection{Comparison of wav2vec~2.0 and Whisper}

\subsubsection{Motivations}

The motivations behind wav2vec~2.0 and Whisper differ, yet they originate from a shared insight, as both address one of the central challenges for modern neural network--based ASR systems, as discussed in section~\ref{sec:challenges_in_asr}: the lack of (very) large labeled datasets.

\citet{baevskiWav2vec20Framework} observe that, in principle, this issue should not prevent the development of highly accurate ASR systems---even in low-resource scenarios where only very small labeled datasets exist, which is the case for many of the world's lesser spoken languages.
They draw this conclusion from the fact that human infants are also able to acquire language understanding without such labeled examples.
Instead, infants listen to adults around them speaking a language and gradually learn useful representations of speech.
Further, \citet{baevskiWav2vec20Framework} argue that self-supervised learning on unlabeled speech data---often readily available, even in many of the mentioned low-resource scenarios---can provide a similar mechanism for machine learning models, since this unsupervised method has already proven itself effective across many NLP tasks and in other fields of research such as computer vision.

They propose training an encoder-only model to learn contextualized representations of jointly learnt quantized speech units.
For downstream tasks such as speech recognition, they add a linear decoder and fine-tune their model on possibly very little labeled data using a Connectionist Temporal Classificiation (CTC) \cite{gravesConnectionistTemporalClassification}.

By contrast, \citet{radfordRobustSpeechRecognition2023} highlight a general limitation of such self-supervised pre-training approaches: while they are able to train audio encoders providing high-quality representations of speech, they lack an equally strong pre-trained decoder, requiring fine-tuning on labeled data for downstream tasks such as ASR; a complex process introducing the risk of the model adepting to dataset-specific patterns and reducing its ability to generalize well to other datasets.
Encoder-only models also often benefit from the use of an additional language model together with the fine-tuned decoder. % add source

\citet{radfordRobustSpeechRecognition2023} instead suggest adopting a large-scale weakly-supervised strategy, moving beyond existing gold-standard labeled datasets and pre-training encoder-decoder models on these substantially larger collections of labeled speech data.
This weakly-supervised approach has already been shown to improve robustness and generalization in other domains, such as computer vision.
For this purpose, they constructed Whisper, a multilingual end-to-end ASR model trained on the largest labeled speech dataset of its time.

\subsubsection{Architectures}

The wav2vec~2.0 model consists of a speech feature encoder building latent speech feature representations directly from the raw input waveform.
It consists of a stack of 1D convolutional layers each followed by layer normalization \cite{baLayerNormalization2016} and a GELU activation function \cite{hendrycksGaussianErrorLinear2023}.

Whisper also uses raw audio waveforms as input, however, it first computes a log mel spectogram of this waveform, normalizes the computed channel values and passes them to a small convolutional feature encoder, similar to the one just described used by \citet{baevskiWav2vec20Framework}.

With wav2vec~2.0, the latent speech features computed by the feature encoder are then passed to the so called context network, consisting, for now, only of the encoder part of a Transformer \cite{vaswaniAttentionAllYou2023}.
This includes multiple blocks of residual connections, self-attention layers, feed-forward networks and layer normalization and serves the purpose of building contextualized representations of speech units, which can later be decoded into a textual transcription by a linear decoder, added to the context network after pre-training.

The output of Whisper's feature encoder is instead passed to an entire encoder-decoder Transformer \cite{vaswaniAttentionAllYou2023}, using pre-activation residual connections and sharing the same width and number of Transformer blocks between encoder and decoder, which are connected through cross attention.

\citet{baevskiWav2vec20Framework} use relative positional embeddings as the positional information used by the encoder.
They compute these embeddings by adding a convolution of the inputs followed by a GELU activation on to the inputs and then applying layer normalization.

\citet{radfordRobustSpeechRecognition2023} opted to use sinosoidal position embeddings for the encoder and learned position embeddings for the decoder instead.

\subsection{Training}

The (pre-)training process is where wav2vec~2.0 and Whisper differ very much.
As stated previously, \citet{baevskiWav2vec20Framework} employ a self-supervised pre-training approach for wav2vec~2.0, while \citet{radfordRobustSpeechRecognition2023} use a supervised approach.

During the pre-training of wav2vec~2.0, random time spans in the input audio are chosen and the corresponding speech feature vectors computed by the feature encoder are masked out, similar to the training methods used in other masked language models such as BERT \cite{devlinBERTPretrainingDeep2019}.
Masked out feature vectors are simply replaced by a trained feature vector shared between all masked time steps.

The masked feature vectors are then passed to the context network, producing contextualized representations of speech for each time step.

A contrastive loss is then calculated for each masked time step:
A quantization module---only used during pre-training---computes quantized speech representations for each of the masked feature vectors by discretizing each unmasked feature to a finite set of speech representations using product quantization \cite{jegouProductQuantizationNearest2011}, an approximate nearest-neighbor method that maps each feature vector to a combination of learned codebook entries.

Each of the quantized representations are then used in a classification problem:
Given the contextualized representation for a certain masked time step, the model must identify the correct quantized speech representation associated with that time step from a set of candidate representations containing distractors.
The distractor representations for this contrastive task are quantized representations sampled from some of the other masked time steps within the same utterance, ensuring that the negative examples are acoustically and contextually plausible.

Rather than employing an explicit classification loss, the cosine similarity scores between the contextualized representation of the masked time step and each candidate representation are used to calculate the contrastive loss, which is then be used by the optimizer.
The contrastive training objective encourages the similarity score of the true target to be higher than those of the distractors, thereby pushing the model to align the contextual representations and the discrete speech units.

\citet{baevskiWav2vec20Framework} also enable the gradients of the contrastive loss to flow back through the quantization module during optimization by using a hard selection for determining the quantized representations in a forward pass and assuming a probabilistic soft selection in the backwards pass of a pre-training step.
As a result, the codebook embeddings used by the quantization module are learned jointly with the rest of the model, allowing the quantized speech units themselves to adapt to the structure of the speech data.

Additionally, \citet{baevskiWav2vec20Framework} employ a diversity loss with the purpose of codebook entries going unused in the quantization module.

Pre-training Whisper is conceptually simpler, since it uses a standard Transformer and sequence-to-sequence cross-entropy loss.

For every transcribed speech sample, the audio is passed through the encoder.
The decoder then autoregressively predicts the next transcript token given all previous correct tokens and while attending to the encoded audio using cross-attention.
% The optimizer then minimizes the cross-entropy loss over...
% TODO: continue here

% TODO: write about fine-tuning

\subsubsection{Fine-Tuning}

After pre-training the encoder in the described fashion, the wav2vec~2.0 model then needs to be fine-tuned for speech recognition.
A linear decoder is added onto the context network, projecting the learned speech representations onto a set number of classes, representing the task specific vocabulary.
This new model is then optimized using labeled speech data and minimizing CTC loss \cite{gravesConnectionistTemporalClassification}.

\subsubsection{Inference}\label{sec:inference}

\begin{comment}

\subsection{Self-Supervised Encoder-only Speech Representation Models}

\subsubsection{Motivation}

As discussed in Section~\ref{sec:challenges_in_asr}, a major challenge for modern neural networkâ€“based ASR systems is the lack of large labeled datasets containing thousands of hours of transcribed speech for most of the world's languages.

\citet{baevskiWav2vec20Framework} note, however, that human infants are indeed able to acquire language understanding without such labeled examples anyway.
Instead, infants listen to adults around them speaking a language and gradually learn useful representations of speech.
Further, \citet{baevskiWav2vec20Framework} argue that self-supervised learning on unlabeled speech data can provide a similar mechanism for machine learning models, since it has proven itself effective across many NLP tasks and in other fields of research such as computer vision.

With wav2vec~2.0, \citet{baevskiWav2vec20Framework} use unlabeled speech data to train an encoder-only model to learn contextualized representations of jointly learnt quantized speech units.
Later on, they add a linear decoder and fine-tune their model on labeled data using a Connectionist Temporal Classificiation (CTC) loss \cite{gravesConnectionistTemporalClassification} to perform tasks such as speech recognition down the line.

\subsubsection{Model Architecture}

Their model consists of a speech feature encoder, consisting of a stack of 1D convolutional layers each followed by layer normalization \cite{baLayerNormalization2016} and a GELU activation function \cite{hendrycksGaussianErrorLinear2023}.
This feature encoder builds latent feature representations for a set number of timesteps directly from the raw input waveform.

The output of the feature encoder is then passed to the so called context network, which in the pre-training phase consists only of the encoder part of a Transformer \cite{vaswaniAttentionAllYou2023}.
For encoding positional information, \citet{baevskiWav2vec20Framework} use relative positional embeddings by adding a convolution of the inputs followed by a GELU activation on to the inputs and then applying layer normalization.

\subsubsection{Self-Supervised Pre-Training}

During pre-training, random time spans in the input audio are chosen and the corresponding speech feature vectors computed by the feature encoder are masked out, similar to the self-supervised training methods used in other masked language models such as BERT \cite{devlinBERTPretrainingDeep2019}.
Masked out feature vectors are simply replaced by a trained feature vector shared between all masked time steps.

The masked feature vectors are then passed to the context network, producing contextualized representations of speech for each time step.

A loss is then calculated for each masked time step, based on a contrastive learning objective.
A quantization module computes quantized speech representations for each of the masked feature vectors by discretizing each unmasked feature to a finite set of speech representations using product quantization \cite{jegouProductQuantizationNearest2011}, an approximate nearest-neighbor method that maps each feature vector to a combination of learned codebook entries.
Each of the quantized representations are then used in a classification problem:
Given the contextualized representation for a certain masked time step, the model must identify the correct quantized speech representation associated with that time step from a set of candidate representations containing distractors.
The distractor representations for this contrastive task are quantized representation sampled from some of the other masked time steps within the same utterance, ensuring that the negative examples are acoustically and contextually plausible.
Rather than employing an explicit classification loss, the cosine similarity scores between the contextualized representation of the masked time step and each candidate quantized representation is used to calculate the contrastive loss.

This training objective encourages the similarity score of the true target to be higher than those of the distractors, thereby pushing the model to align contextual representations with the learnt discrete speech units.
Since the masked input provides no direct acoustic evidence, success on this task requires the model to learn representations that capture meaningful contextual information from the surrounding speech.

Finally, because the contrastive loss includes the quantized speech representations, gradients can propagate back through the quantization module during training.
As a result, the codebook embeddings used for quantization are learned jointly with the rest of the model, allowing the quantized speech units themselves to adapt to the structure of the speech data.

\subsubsection{Fine-Tuning}

After pre-training, the model is then fine-tuned for speech recognition.
A linear decoder is added onto the context network, projecting the learned speech representations onto a set number of classes, representing the task specific vocabulary.
This new model is then optimized using labeled speech data and minimizing CTC loss \cite{gravesConnectionistTemporalClassification}.

%TODO: should i explain how CTC loss works?

\subsubsection{Results}

\subsubsection{Connectionist Temporal Classificiation Loss}


\subsubsection{Conclusions}

% con: CTC makes strong independence assumption
% -> leads to what errors?
% bidirectional attention defeats this assumption?
% balanced out by applying LLM how?

% encoder-only models/masked language models are generally used for interpretative tasks instead of generation

% attention looks backwards AND forwards -> only applicable on offline data?

\citet{baevskiWav2vec20Framework} conclude that pre-training speech recognition models on unlabeled data shows large potential, especially for ultra--low resource scenarios with very little labeled speech data available.
This could improve the availability of speech recognition systems for many lesser spoken languages as well as dialects, as this still remains a challenge in ASR (see section \ref{sec:challenges_in_asr}).

% TODO: compare size of models

\subsection{Attention-based Encoder-Decoder Architecture}

% make it clear, that whisper improves on wav2vec 2.0, without the unsupervised techniques
Due to the unavailability of high-quality large labeled datasets for ASR in many languages, self-supervision techniques such as wav2vec 2.0 \cite{baevskiWav2vec20Framework} made use of the large amounts of readily available unlabeled speech data, achieving astounding success.
Separately, research in other fields such as computer vision showed that training on much larger weakly supervised datasets, rather than only high-quality labeled data, leads to significant improvements in robustness and generalization of models \cite{radfordRobustSpeechRecognition2023}.

% TODO: make clear, that it was a huge feat to construct these new large datasets
Motivated by these trends, \citet{radfordRobustSpeechRecognition2023} identified a key limitation of ASR models such as wav2vec~2.0, which only use a pre-trained encoder: whilst they learn powerful speech representations, they lack an equivalently strong pre-trained decoder and have to therefore rely heavily on fine-tuning to perform tasks such as speech recognition.
\citet{radfordRobustSpeechRecognition2023} instead pre-trained an entire encoder-decoder Transformer on large amounts ($680{,}000$ hours) of weakly supervised labeled speech data, successfully achieving a new state of the art with the Whisper models.
% no need for self-supervision, instead relying on more weakly-supervised data
% wav2vec 2.0 better for low-resource languages?

% whisper doesn't use (inverse) text normalization, except when filtering out bad transcripts


% supervised fine-tuning after unsupervised pre-training in wav2vec 2.0/weakly-supervised pre-training in whisper
% convolutions as preprocessing step
% pre-activation residual blocks by having layer norm applied first

\end{comment}

% positional encoding

% in whisper:
% 1d convolutional layers compress input sequence and compute speech features

% talk about WER calculation by applying text normalization first

% general trends:
% more capable models (e.g. translation, multi-language in whisper) trained on even more (labeled) data
% unsupervised learning in (ultra-)low-resource scenarios?
% transformer has emerged as superior architecture
% more of the ASR pipeline built directly into models, such as inverse text normalization in whisper
% more joint learning e.g. of representations, positional encodings, attention

\bibliography{zotero}

\end{document}
