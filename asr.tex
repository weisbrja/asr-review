\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[english]{babel}
% TODO: is this correct and how to achieve similar thing for wav2vec 2.0?
\hyphenation{OpenAI}
\hyphenation{LibriSpeech}

\usepackage{siunitx}
\usepackage{mathptmx}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{comment}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{csquotes}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage[absolute,overlay]{textpos}
\usepackage{xcolor}
\usepackage{enumitem}

\title{A Review of Automatic Speech Recognition}

\author{Jannis Weisbrodt \\
  Karlsruhe Institute of Technology \\
  \texttt{jannis.weisbrodt@student.kit.edu}
  }

\begin{document}

\maketitle

% TODO: write abstract
\begin{abstract}
Abstract goes here.
\end{abstract}

\section{Introduction}

Automatic Speech Recognition (ASR) is the task of transcribing spoken language into writing---one of the earliest goals of computer language processing \cite{jurafskySpeechLanguageProcessing2025}---and represents a subfield within natural language processing (NLP).
Today, automatic transcription is used ubiquitously not only in fields like law, where dictation is essential, but also in voice-controlled assistants as well as automatic captioning for movies, videos, conferences or voice messages.
Advances in ASR not only enhance accessibility for individuals with typing or hearing impairments, but also generally provide a more natural interface for interacting with modern machines.
Despite the progress, accurate automatic transcription of speech by \emph{any speaker} in \emph{any environment} in an \emph{open vocabulary} setting still remains a challenging problem that is far from solved.

In this review, we begin by introducing the ASR task and outlining the challenges involved in producing accurate transcriptions of speech.
We then examine how ASR systems process speech, covering common techniques and the architectures underlying modern ASR models.
Next, we focus on two representative ASR systems, describing and comparing their architectures, training procedures and inference processes, before discussing evaluation and their performance.
We conclude by summarizing the insights into modern computer-based speech recognition provided by these models, as well as outlining remaining open questions and ongoing challenges in the field of ASR.

\section{Challenges in ASR}\label{sec:challenges_in_asr}

The complexity of the ASR task can be factored along multiple dimensions that each affect transcription accuracy.

\paragraph{Vocabulary Size}
One of the main challenges in ASR is the potentially very large vocabulary that needs to be recognized and accurately transcribed.

While some low vocabulary tasks such as digit recognition or detecting \enquote{yes} versus \enquote{no} answers have been effectively solved for some time, transcribing unconstrained speech with a large general vocabulary---such as in videos or conversations---remains considerably more difficult \cite{jurafskySpeechLanguageProcessing2025}.

As vocabulary size grows, so do lexical ambiguity and the likelihood of rare or even unseen words, especially in a multilingual context or in the presence of domain-specific language, such as medical, legal or technical terminology.
Code-switching---switching between multiple languages within the same utterance---further increases this complexity.

\paragraph{Acoustic and Speaker Variability}
Beyond vocabulary size, ASR must account for the fact that the same sentence can be uttered in many acoustically distinct ways.
Differences in dialect or accent, speaking rate, prosody, and vocal tract characteristics lead to substantial variation in the speech signals produced by different speakers, even when the linguistic content is identical.
This increases acoustic ambiguity and inherently complicates reliable speech recognition.

Accents and dialects---especially those associated with lesser spoken languages---pose a major challenge for ASR systems.
For many of these local varieties, large labeled speech corpora simply do not exist.
These accents and dialects are therefore often severely underrepresented during training, which can lead to models struggling to recognize them reliably, resulting in significantly higher error rates for affected speaker groups \cite{jurafskySpeechLanguageProcessing2025}.
Robust ASR therefore requires models that work well even in low-resource scenarios, which is quite a challenge \cite{baevskiWav2vec20Framework2020}. % this sounds weird

Apart from variability in acoustics across speakers, spoken language frequently contains disfluencies, such as fragmented, repeated or truncated words (e.g. \enquote{main- mainly}), as well as filled pauses (e.g. \enquote{uhm}, \enquote{uh}).
These speech specific phenomena deviate from canonical word forms and introduce additional uncertainty when transcribing.

\paragraph{Audio Quality and Recording Environment}

A further dimension of complexity arises from the recording conditions under which speech is captured.
Audio recorded in a quiet studio environment with a high quality microphone placed close to the speaker is substantially easier to transcribe than speech recorded in noisy, everyday settings such as busy streets, restaurants or echoing rooms.

Background noise and competing speakers can mask and distort parts of the already complex speech signal.
Reverberation in enclosed spaces can blur sounds over time, making it harder to distinguish individual words or phonemes.
The quality and placement of the recording microphone, as well as compression or transmission artifacts, can further distort the signal.

All of these factors make speech recognition under real-world conditions significantly more difficult.

\section{Approaches to ASR}

Historically, a wide range of approaches have been developed and applied to the ASR task.
In this section, we provide an overview of modern ASR systems, highlighting the use of neural network--based model architectures, and in particular the dominance of Transformer-based models.
We then examine and compare two representative systems---wav2vec~2.0 \citep{baevskiWav2vec20Framework2020} and OpenAI's Whisper \citep{radfordRobustSpeechRecognition2023}---discussing architectures, training, inference and evaluation of their performance. % TODO: add further points discussed in this section here.

\subsection{Common Techniques in ASR}

Before examining these specific speech recognition models, it is useful to review techniques commonly used across both the reference and other ASR systems.

\paragraph{ASR Pipeline}
Speech recognition systems often times consist of multiple components, built around a core speech recognition model.
Most of the time, these components are arranged as a pipeline and can include audio format conversion, feature extraction, subsampling, voice activity detection and inverse text normalization \cite{radfordRobustSpeechRecognition2023}.

\subsubsection{Speech Feature Extraction}

ASR systems often have exact requirements on the format of the audio data they process.

\paragraph{Raw Waveform}
The most basic representation of audio data, used for example by wav2vec~2.0, is referred to as the raw waveform.
This format represents audio data simply by the amplitude of the recorded sound waves at each time frame.
These amplitudes are digitized by sampling them at a certain sampling frequency and quantizing them to a certain bit depth.

Since speech recognition systems typically require their audio input to have a fixed sampling frequency (e.g.\ $\SI{16}{\kilo\hertz}$ for wav2vec 2.0 and Whisper) and a fixed bit depth, recorded audio data often needs to be resampled and requantized before it can be processed.

\paragraph{Log Mel Spectrum}
Other ASR models, such as Whisper, operate on a different audio representation and therefore require an additional preprocessing step.
Instead of processing the raw waveform directly, the system first computes the so called log mel spectrum for small windows of time (characterized by the window size, stride and shape) and uses the resulting feature vectors as the input.

The mel is a unit of pitch representing the human perception of changes in pitch (which is roughly linear for frequencies between $\SI{100}{\hertz}$ and $\SI{1000}{\hertz}$ and logarithmic for frequencies above this range) more accurately than raw frequency values.
By computing the frequencies present in the audio signal for each time window using a Fourier transform, we can sample the mel spectrum values for this time window by accumulating the energy levels of each frequency range using a mel filter bank.
Because human perception of amplitude is also logarithmic, we then take the log of each of the mel spectrum values, resulting in a vector of scalars, each of which is called a channel \cite{jurafskySpeechLanguageProcessing2025}.

\paragraph{Normalization}
It is also common for modern ASR systems to normalize their audio input.
\citet{baevskiWav2vec20Framework2020} rescale the input of wav2vec~2.0 to have zero mean and unit variance, while \citet{radfordRobustSpeechRecognition2023} normalize the computed channels to lie within the range $[-1,1]$ and to also have approximately zero mean before processing them with Whisper.

\subsubsection{Text Normalization}\label{sec:text_normalization}

Historically, ASR systems were often trained to produce normalized transcriptions, stripped of any punctuation and entirely upper or lower case, using standardized spelling and with dates, numbers and symbols turned into a standardized form, e.g.\ \enquote{25} into \enquote{twenty-five} or \enquote{\$6} into \enquote{six dollars}.
To actually produce naturalistic transcripts, these systems therefore often implemented an inverse mapping of the standardized transcriptions back to naturalistic text, possibly applying a specific locale in the process.
This step is often accordingly called inverse text normalization.

With Whisper, \citet{radfordRobustSpeechRecognition2023} demonstrated that modern end-to-end ASR architectures are capable of directly learning the mapping between utterances and text transcripts, eliminating the need for such explicit text or inverse text normalization, thereby simplifying the speech recognition pipeline.

\subsection{Neural Network--Based Model Architectures}

Now that we have discussed some of the pre- and post-processing steps commonly included in ASR systems, we turn to the main component of modern ASR pipelines:
a neural network responsible for converting speech features into output tokens.

More formally, the speech recognition model solves the task of computing the conditional probability $p(\mathbf{y}\mid\mathbf{x})$ of a sequence of linguistic units $\mathbf{y}=(y_1,\ldots,y_m)$ given the extracted speech features $\mathbf{x}=(x_1,\ldots,x_n)$.

As in many other machine learning domains---particularly in NLP---the model is typically implemented using neural networks.
Over time, various neural architectures have been applied, including feedforward, convolutional, and recurrent models.
In recent years, many state-of-the-art systems have increasingly adopted attention-based designs.

\subsubsection{Transformer Architecture}

In particular, the Transformer architecture, proposed by \citet{vaswaniAttentionAllYou2023}, now forms the prevalent foundation used by most state-of-the-art ASR systems, reflecting a broader trend in machine learning.

The Transformer architecture uses a self-attention mechanism to compute context-aware representations for each element in the input sequence by combining information from all positions, enabling the model to capture dependencies across the entire input sequence.

\paragraph{Self-Attention}
Self-attention is computed by projecting each element of the input sequence is into query, key and value vectors, and using the dot-product similarity between a query and all keys to produce attention scores, which are then normalized and used as weights to sum the corresponding value vectors.

\paragraph{Multi-Head Attention}
To capture multiple types of relationships in parallel, a single self-attention layer deploys multiple attention heads, where each head has its own projections for computing query, key and value vectors and performs the attention calculation independently; the outputs of all attention heads are then concatenated and projected into a single contextualized representation.

\paragraph{Transformer Blocks}
These multi-headed self-attention computations as well as an additional feedforward network are then combined into a single Transformer block, each of them residually adding context to each input vector.

\paragraph{Encoder-Decoder Architecture}
Stacking such blocks can be used to create an encoder that transforms each feature vector of the input sequence into a contextualized representation.

A decoder can be built in a similar way, however with self-attention being causal (left-to-right) instead.
The first decoder block is then given all previous output tokens as input and in every decoder block, a feature vector may only attend to previous positions.

\paragraph{Cross-Attention}
Additionally, through cross-attention, the decoder can attend to the entire sequence of feature vectors generated by the encoder by using the outputs of the previous decoder block as queries, while the keys and values come from the encoder output.

\paragraph{Advantages}
The Transformer architecture outperformed earlier speech recognition models, including recurrent networks, primarily because it captures long-range temporal dependencies in the input audio more effectively while simultaneously enabling parallel computation across the input sequence.

This has since made it a prevalent building block in many state-of-the-art ASR systems, including wav2vec~2.0 and Whisper.

\section{Comparison of ASR models}\label{sec:comparison_of_models}

Having outlined the modern approach to ASR, in which neural networks---particularly Transformer-based architectures---act as the core component of the speech recognition system, we now examine two representative models that illustrate different paradigms of contemporary Transformer-based ASR:
wav2vec~2.0 \citep{baevskiWav2vec20Framework2020}, a self-supervised encoder-only model fine-tuned with CTC loss, and OpenAI's Whisper \citep{radfordRobustSpeechRecognition2023}, a weakly-supervised encoder-decoder model.

\subsection{Motivations}

The motivations behind wav2vec~2.0 and Whisper differ, yet they originate from a shared insight, as both address one of the central challenges for modern neural network--based ASR systems, as discussed in Section~\ref{sec:challenges_in_asr}: the lack of (very) large labeled datasets.

\citet{baevskiWav2vec20Framework2020} observe that, in principle, this issue should not prevent the development of highly accurate ASR systems---even in low-resource scenarios where only very small labeled datasets exist, which is the case for many of the world's lesser spoken languages.
They draw this conclusion from the fact that human infants are also able to acquire language understanding without such labeled examples.
Instead, infants listen to adults around them speaking a language and gradually learn useful representations of speech.
Further, \citet{baevskiWav2vec20Framework2020} argue that self-supervised learning on unlabeled speech data---often readily available, even in many of the mentioned low-resource scenarios---can provide a similar mechanism for machine learning models, since this self-supervised method has already proven itself effective across many NLP tasks and in other fields of research such as computer vision.

They propose training an encoder-only model to learn contextualized representations of jointly learnt quantized speech units.
For downstream tasks such as speech recognition, they add a linear decoder and fine-tune their model on possibly very little labeled data using a Connectionist Temporal Classificiation (CTC) \cite{gravesConnectionistTemporalClassification}.

By contrast, \citet{radfordRobustSpeechRecognition2023} highlight a general limitation of such self-supervised pre-training approaches: while they are able to train audio encoders providing high-quality representations of speech, they lack an equally strong pre-trained decoder, requiring fine-tuning on labeled data for downstream tasks such as ASR; a complex process introducing the risk of the model adepting to dataset-specific patterns and reducing its ability to generalize well to other datasets.
Encoder-only models also often benefit from the use of an additional language model together with the fine-tuned decoder.

\citet{radfordRobustSpeechRecognition2023} instead suggest adopting a large-scale weakly-supervised strategy, moving beyond existing gold-standard labeled datasets and pre-training encoder-decoder models on these substantially larger collections of labeled speech data.
This weakly-supervised approach has already been shown to improve robustness and generalization in other domains, such as computer vision.
For this purpose, they constructed Whisper, an end-to-end ASR model trained on the largest labeled speech dataset of its time, with the goal of achieving robust transcription of multilingual speech in a zero-shot setting, meaning without any fine-tuning.

\subsection{Architectures}

The wav2vec~2.0 model consists of a speech feature encoder building latent speech feature representations directly from the raw input waveform.
It consists of a stack of 1D convolutional layers each followed by layer normalization \cite{baLayerNormalization2016} and a GELU activation function \cite{hendrycksGaussianErrorLinear2023}.

Whisper also uses raw audio waveforms as input, however, it first computes a log mel spectogram of this waveform, normalizes the computed channel values and passes them to a small convolutional feature encoder, similar to the one just described used by \citet{baevskiWav2vec20Framework2020}.

With wav2vec~2.0, the latent speech features computed by the feature encoder are then passed to the so called context network, consisting, for now, only of the encoder part of a Transformer \cite{vaswaniAttentionAllYou2023}.
This includes multiple blocks of residual connections, self-attention layers, feed-forward networks and layer normalization and serves the purpose of building contextualized representations of speech units, which can later be decoded into a textual transcription by a linear decoder, added to the context network after pre-training.

The output of Whisper's feature encoder is instead passed to an entire encoder-decoder Transformer \cite{vaswaniAttentionAllYou2023}, using pre-activation residual connections and sharing the same width and number of Transformer blocks between encoder and decoder, which are connected through cross attention.

\citet{baevskiWav2vec20Framework2020} use relative positional embeddings as the positional information used by the encoder.
They compute these embeddings by adding a convolution of the inputs followed by a GELU activation on to the inputs and then applying layer normalization.

\citet{radfordRobustSpeechRecognition2023} opted to use sinosoidal position embeddings for the encoder and learned position embeddings for the decoder instead.

\subsection{Training}\label{sec:training}

The training process is where wav2vec~2.0 and Whisper differ very much.

As stated previously, \citet{baevskiWav2vec20Framework2020} employ a self-supervised pre-training approach for wav2vec~2.0, while \citet{radfordRobustSpeechRecognition2023} use a supervised approach.

\paragraph{Pre-Training Dataset}
The approaches not only differ in paradigm, but also in the data used to pre-train the models.
For the self-supervised pre-training of wav2vec~2.0, \citet{baevskiWav2vec20Framework2020} use the LibriSpeech corpus \cite{panayotovLibrispeechASRCorpus2015}, containing $\SI{960}{\hour}$ of unlabeled data, as well as the larger LibriVox corpus \cite{kahnLibriLightBenchmarkASR2020}, amounting to $\SI{53.2}{\kilo\hour}$ of audio data.
Instead of using an existing corpus, \citet{radfordRobustSpeechRecognition2023} curate $\SI{680}{k\hour}$ of labeled audio data from publicly accessible sources on the internet, including multilingual data, amounting to one order of magnitude more labeled data than previous attempts.

% TODO: insert paragraphs here
During the pre-training of wav2vec~2.0, random time spans in the input audio are chosen and the corresponding speech feature vectors computed by the feature encoder are masked out, similar to the training methods used in other masked language models such as BERT \cite{devlinBERTPretrainingDeep2019}.
Masked out feature vectors are simply replaced by a trained feature vector shared between all masked time steps.

The masked feature vectors are then passed to the context network, producing contextualized representations of speech for each time step.

A contrastive loss is then calculated for each masked time step:
A quantization module---only used during pre-training---computes quantized speech representations for each of the masked feature vectors by discretizing each unmasked feature to a finite set of speech representations using product quantization \cite{jegouProductQuantizationNearest2011}, an approximate nearest-neighbor method that maps each feature vector to a combination of learned codebook entries.

Each of the quantized representations are then used in a classification problem:
Given the contextualized representation for a certain masked time step, the model must identify the correct quantized speech representation associated with that time step from a set of candidate representations containing distractors.
The distractor representations for this contrastive task are quantized representations sampled from some of the other masked time steps within the same utterance, ensuring that the negative examples are acoustically and contextually plausible.

Rather than employing an explicit classification loss, the cosine similarity scores between the contextualized representation of the masked time step and each candidate representation are used to calculate the contrastive loss, which is then be used by the optimizer.
The contrastive training objective encourages the similarity score of the true target to be higher than those of the distractors, thereby pushing the model to align the contextual representations and the discrete speech units.

\citet{baevskiWav2vec20Framework2020} also enable the gradients of the contrastive loss to flow back through the quantization module during optimization by using a hard selection for determining the quantized representations in a forward pass and assuming a probabilistic soft selection in the backwards pass of a pre-training step (Gumbel softmax).
As a result, the codebook embeddings used by the quantization module are learned jointly with the rest of the model, allowing the quantized speech units themselves to adapt to the structure of the speech data.

Additionally, \citet{baevskiWav2vec20Framework2020} employ a diversity loss with the purpose of preventing codebook entries from going unused in the quantization module.

Pre-training Whisper is conceptually simpler, since it uses a standard Transformer and sequence-to-sequence cross-entropy loss.

With Whisper, the audio of a transcribed speech sample is first passed through the encoder.
The decoder then autoregressively predicts the next transcript token given all previously correct tokens and whilst attending to the encoded audio using cross-attention, similar to inference as described in Section~\ref{sec:inference}.

The optimizer then minimizes the cross-entropy loss over the generated token.
This is done for all expected transcript tokens.

Both models also employ (a modified version of) SpecAugment \citet{parkSpecAugmentSimpleData2019} to augment labeled training data, with wav2vec~2.0 using it during fine-tuning (Section~\ref{sec:fine-tuning}).

% TODO; describe whisper's multi-task training: translation, transcription for multiple languages, voice activity detection, speaker diarization, time stamps

% TODO: talk about how alignment is learned

\subsection{Fine-Tuning}\label{sec:fine-tuning}

Pre-training a wav2vec~2.0 model only teaches it to produce contextualized speech representations rather than actual transcript text.
To perform downstream speech recognition, a initially randomized linear layer is appended onto the context network of the pre-trained encoder-only model, mapping each contextualized representation to a logit vector.
This is then followed by a softmax to produce a probability distribution over the output vocabulary, e.g.\ characters or phonemes as well as a special CTC boundary symbol.
Because the layer is added after pre-training, it necessitates a fine-tuning step for adapting its weights to perform ASR.

% TODO: talk about what labeled data wav2vec 2.0 is fine-tuned on

In contrast, Whisper is already trained on large amounts of labeled speech samples and can perform ASR in zero-shot setting, making fine-tuning optional---one of the research goals of \citet{radfordRobustSpeechRecognition2023}.

The actual fine-tuning of wav2vec~2.0 performed by \citet{baevskiWav2vec20Framework2020} uses CTC loss \cite{gravesConnectionistTemporalClassification} with some (possibly very little) labeled speech data.
The learned feature encoder stays fixed and is not trained during this process.
Latent speech features produced by it are however tampered with during fine-tuning using a method similar to SpecAugment \cite{parkSpecAugmentSimpleData2019}, as previously stated, because this improves generalization and robustness.

Fine-tuning the decoder using CTC loss has the goal of maximizing the probability of the model outputting the correct transcript, when all duplicate output tokens, e.g.\ characters or phonemes, between CTC boundary tokens are collapsed, as this collapse also happens during inference (Section~\ref{sec:inference}).
Different alignments of the input audio (or the generated speech representations) and the output tokens are therefore explicitly not considered, as they all amount to the same transcript being generated.

The CTC loss thus marginalizes over all valid alignments of output tokens---the sequences that collapse to the ground-truth transcript.
This marginalization is efficiently computed using dynamic programming, summing the probabilities of all such alignments without explicitly enumerating and calculating the probability of each alignment.
The dynamic programming approach used here takes advantage of the fact that alignments producing the same prefix of the target transcription at the same time step in the input audio share the same accumulated probability up to that point.

One of the main issues in training a decoder using CTC loss, as done in wav2vec~2.0, is that CTC relies on a strong conditional independence assumption.
Every output token $y_i$ in the output sequence $\mathbf{y}=(y_1, \ldots, y_m)$ is assumed to be conditionally independent of every other output token $y_j$ given the input sequence $\mathbf{x}$ of audio speech features:
\begin{equation*}
  p(\mathbf{y}\mid\mathbf{x}) = \prod_{t=1}^m p(y_t\mid\mathbf{x})
\end{equation*}

\noindent
As a result, the layer added onto a pre-trained wav2vec~2.0 model cannot capture sequential dependencies between output tokens and does therefore not learn an implicit language model, unlike Whisper, which does not make this assumption of conditional independence.

Not learning an implicit language model can partially be overcome by including an external language model when decoding (Section~\ref{sec:inference}).
It is even possible for this approach to prove beneficial, for example when transferring between different ASR domains:
even though the language used in the domains might differ, the acoustics of speech often still remain similar.
With a decoder trained only using CTC, a new language model can then simply be swapped in to adapt to the new domain \cite{jurafskySpeechLanguageProcessing2025}.

\subsection{Inference}\label{sec:inference}

Both Whisper and wav2vec~2.0 convert the latent representations produced by their respective encoders into output tokens for inference.

Whisper's decoder generates BPE tokens, necessitating a mapping step to achieve a human readable transcript, while wav2vec~2.0 already generates characters (or phonemes) but still requires the CTC collapse described in Section~\ref{sec:fine-tuning} to generate the final transcript.

The biggest difference in inference between wav2vec~2.0 and Whisper is due to the conditional independence assumption made only by the former, but not the latter.

Because \citet{baevskiWav2vec20Framework2020} assume output tokens to only depend on the latent representations generated by the encoder and be independent of each other, inference of an output token for each time step of the input audio can be done in parallel.
The CTC collapse then only requires a further linear pass over the generated output tokens.

For Whisper, inference happens autoregressively instead.
When generating a new output token, the decoder not only attends to the latent representations of the encoder through cross attention, but also to all previously generated output tokens---each using a learned positional encoding---using causal (left-to-right) self-attention.
This autoregessive property causes inference to be inherently sequential, making it computationally less efficient than the inference for wav2vec~2.0.

Because the decoders of both Whisper and wav2vec~2.0 output a probability distribution over the possible output tokens rather than a single concrete token, this distribution must then be converted into an actual prediction via a decoding strategy.
This can be done, for example, by greedy decoding, which selects the most probable token, or by beam search.

Beam search works by keeping the most probable candidate sequences of output tokens in a search beam.
At each step, every sequence in the beam is extended by all possible next output tokens, the probabilities of the resulting sequences are calculated by accumulating output token probabilities computed by the model, and then only the most probable sequences are retained for the next step.

This decoding strategy incurs additional computational cost compared to greedy decoding, but it also enables the incorporation of an external language model in the decoding process.

This can be achieved by rescoring each candidate sequence in the beam, interpolating the probability assigned by the external language model with the probability computed by the decoder using a certain weighting factor.
Additionally, since language models tend to prefer shorter sequences, a length normalization term is also included in this rescoring.

% TODO: add references to evaluation of incorporating LLMs for inference
Since wav2vec~2.0 does not learn an implicit language model (Section~\ref{sec:fine-tuning}), integrating such an external language model (at least for transcript generation) is particularly beneficial \cite{baevskiWav2vec20Framework2020}.
For Whisper, the usage of external language models can still improve transcript quality, as its implicit language model is trained on substantially less text data than modern LLMs \cite{jurafskySpeechLanguageProcessing2025}.

% TODO: different tasks: how are they specified in whisper?

\subsection{Evaluation}

After describing the architecture, training, and inference procedures of wav2vec~2.0 and Whisper, we now turn to methods for assessing and comparing the transcription accuracy of such ASR systems.

\paragraph{Test Dataset}
As with machine learning models in general, ASR performance is typically evaluated on a held-out test set, normally drawn from the same distribution as the training data (e.g.\ LibriSpeech test-clean/test-other \cite{panayotovLibrispeechASRCorpus2015} for wav2vec~2.0).
Evaluation is performed using standardized evaluation metrics to ensure consistency across studies.

\paragraph{Word Error Rate}
The most widely used metric for ASR is Word Error Rate (WER), which quantifies the difference between the generated transcription and the reference by computing the minimum word-level edit distance between the two sequences, normalized by the total number of words in the reference.

\paragraph{Edit Distance}
The minimum edit distance of two sequences is defined as the minimum number of insertions, deletions, and substitutions required to transform one sequence into the other.

It can be efficiently computed via a dynamic programming approach that calculates the minimum edit distance for all pairs of prefixes of the two sequences, building up to the total minimum edit distance by incrementally computing the edit distances between longer prefixes from the previously computed distances.

\paragraph{Advantages of WER}
WER is popular in speech processing due to its simplicity while being largely language-agnostic---though for some languages, alternatives such as character-level metrics may be more appropriate.
Its straightforward computation ensures consistency and reproducibility of WER scores across studies, providing a clear and easily interpretable measure of transcription accuracy, where lower values indicate fewer transcription errors.

\paragraph{Limitations of WER}
Despite its usefulness, WER has several notable limitations.
First of all, it ignores semantic context, treating all word errors equally regardless of their impact on the overall meaning of the transcription.
This causes small errors such as transcribing \enquote{piece} instead of \enquote{pieced} to be penalized the same as transcribing an entirely different word.

WER is also highly sensitive to variations in transcript style, such as punctuation or spelling variations, which can lead to artificially higher error scores when the style of the reference transcriptions in the test set differs from the style implicitly learned by the model.

\paragraph{Fine-Tuning before Evaluation}
To circumvent the latter problem, ASR models are often fine-tuned on a development test set, which contains transcriptions similar in style to those found in the actual test set.
While fine-tuning on such a development set improves comparability of model performance for each specific test set, measuring the performance of models after fine-tuning them to each test set inherently changes what is being measured by the evaluation.

\paragraph{In-Distribution vs.\ Out-of-Distribution Evaluation and Robustness}
Evaluating WER scores of models that are adapted to data sampled from the same distribution as the test set primarily reflects their ability of in-distribution generalization.

\citet{radfordRobustSpeechRecognition2023} argue that this setting differs fundamentally from how human performance is typically evaluated on the same speech recognition tasks, since humans are often given little to no data from the test distribution prior to evaluation.
Their performance is instead assessed in many different settings without prior fine-tuning.
This measures their out-of-distribution generalization, meaning their ability to maintain a low WER across different testing conditions, which in the context of ASR systems is commonly referred to as robustness.

When evaluating ASR systems that match or even outperform humans on individual benchmarks in a similar manner across different test sets, \citet{radfordRobustSpeechRecognition2023} observed large drops in performance, highlighting a lack of robustness when comparing to humans.

\paragraph{Evaluating Robustness}
To capture the so called overall robustness of an ASR system, \citet{radfordRobustSpeechRecognition2023} measure the average WER of models across a suite of heterogeneous test sets.

Because this overall robustness is directly influenced by a higher base accuracy, they also measure the effective robustness, as introduced by \citet{taoriMeasuringRobustnessNatural2020}, which instead reflects the out-of-distribution performance relative to what would be expected based on in-distribution accuracy.
A model with high effective robustness exhibits degradation in performance smaller than expected and comes closer to the ideal of equal performance across all datasets.

In practice, \citet{radfordRobustSpeechRecognition2023} use LibriSpeech to serve as an anchor for in-distribution performance, with additional test sets evaluated relative to this baseline.

\paragraph{Zero-Shot Evaluation and Text Normalization instead of Fine-Tuning}
To solve the issue of WER inflating scores because of small stylistic mismatches in transcriptions without needing to rely on fine-tuning to the transcription style of each test set, \citet{radfordRobustSpeechRecognition2023} instead use a zero-shot approach when evaluating and instead apply dataset-specific text normalization (Section~\ref{sec:text_normalization}) before calculating WER to produce semantically more accurate scores, that do not penalize minor differences in transcription style.

This approach, however, has the downside of reducing the direct comparability of WER values across different studies.

\subsubsection{Performance on LibriSpeech and other Benchmarks}

Now that we have discussed the advantages and limitations of WER, as well as the robustness measure used by \citet{radfordRobustSpeechRecognition2023}, we will see how these metrics apply to wav2vec~2.0 and Whisper models and compare the results.

With wav2vec~2.0, \citet{baevskiWav2vec20Framework2020} trained two different model configurations: a \textsc{Base} model with 95 million parameters and a \textsc{Large} model with 317 million parameters.

After pre-training such models on either the $\SI{960}{\hour}$ of LibriSpeech audio or the roughly $\SI{60}{\kilo\hour}$ of audio data from LibriVox as described in Section~\ref{sec:training}, \citet{baevskiWav2vec20Framework2020} consider five datasets for fine-tuning each model.

They simulate low-resource labeled data scenarios using test sets from Libri-light \cite{kahnLibriLightBenchmarkASR2020}, with labeled data ranging from as little as $\SI{10}{\minute}$, $\SI{1}{\hour}$ and $\SI{10}{\hour}$ up to $\SI{100}{\hour}$, as well as fine-tuning on the full LibriSpeech corpus ($\SI{960}{\hour}$ of labeled data).

They also investigate the usage of external language models (Section~\ref{sec:fine-tuning}) by pairing each fine-tuned model with some combination of no language model, a 4-gram language model or a Transformer-based language model, with both language models being trained on the LibriSpeech text corpus.

They find that the \textsc{Large} model trained on the $\SI{60}{\kilo\hour}$ of unlabeled audio data from LibriVox paired with the Transformer-based language model only needs fine-tuning on $\SI{10}{\minute}$ of labeled data to achieve WER of $5.2/8.6$ on LibriSpeech's test-clean/test-other test sets, with the former containing primarily high-quality, low-noise recordings with clear pronunciation, while the latter includes more challenging conditions such as background noise, reverberation and accented speech, which make speech recognition substantially more difficult.

These results represent both an impressive feat in speech processing using self-supervised learning, massively improving on previous work and further, they showed that effective speech recognition is even possible in ultra-low resource scenarios, possibly enabling speech recognition for many lesser spoken languages or dialects.

Increasing the amount of unlabeled audio data ($\SI{960}{\hour}$ vs. $\SI{60}{\kilo\hour}$) shows large improvements in performance, demonstrating that self-supervised speech recognition can be significantly improved by training on more unlabeled data.

Increasing model size from \textsc{Base} to \textsc{Large} also improves WER in all tested scenarios.

In a high-resource labeled data scenario, wav2vec~2.0 also proves effective, as the method's best WER of $1.8/3.3$ is achieved by the \textsc{Large} model pre-trained on the full LibriVox corpus ($\SI{60}{\kilo\hour}$ of unlabeled audio) and fine-tuned on the most amount of labeled data (the full $\SI{960}{\hour}$ from the LibriSpeech corpus) when paired with the Transformer-based language model.
This demonstrates that self-supervised learning for speech recognition is not only applicable in low-resource scenarios, rather it also compares well against supervised methods in high-resource scenarios.

\paragraph{Distribution Shift and Robustness}
\citet{radfordRobustSpeechRecognition2023} showed however, that even though wav2vec~2.0 models perform well on LibriSpeech after being fine-tuned, applying them to different data distributions by directly transferring to other test sets leads to a significant increase in WER, therefore demonstrating low effective robustness under distribution shift.

With Whisper, even though the best zero-shot Whisper model achieves a significantly higher WER of $2.5/5.2$ on LibriSpeech, a very different robustness across different test sets is achieved, with WER values reduced by $55.2\%$ on average per test set when compared to the WER of a wav2vec~2.0 model that performs within $0.1\%$ on LibriSpeech.

\citet{radfordRobustSpeechRecognition2023} find that training on the full $\SI{680}{\kilo\hour}$ of semi-labeled audio data collected by them leads to the most robust and best performing Whisper model, with more data strongly correlating to a better perfomance, a common conclusion drawn by both Whisper and wav2vec~2.0.

They, however, start to observe diminishing returns when increasing dataset size, which they explain by saturation effects when approaching human-level performance.

When increasing model size, they also observe an increase in performance, similar to \cite{baevskiWav2vec20Framework2020}.
For English speech recognition, however, they note diminishing returns similarly explained by approaching human-level performance.

\subsection{Conclusions}

\paragraph{Multitask and Multilingual Speech Recognition}
\citet{radfordRobustSpeechRecognition2023} conclude that even though training a single model to perform multiple tasks such as speech recognition, translation into English, producing timestamps for alignment as well as doing language identification, all while being multilingual, might seem contraproductive (better word here pls) because of interference between tasks and languages, resulting in a worse overall performance, but this is not the case, at least for large models trained on a huge dataset.
To the contrary, \citet{radfordRobustSpeechRecognition2023} find that, at a large scale, such multitask and multilingual joint models scale better and outperform similar models trained on English-only data.

In total, they conclude that supervised learning on extremely large semi-supervised datasets has been underappreciated in ASR, and may provide a way forward towards more robustness and even better (multilingual) speech recognition.
% TODO: Whisper as an end-to-end system, rather than needing a custom pipeline with different components -> represents larger trend in ASR

\citet{baevskiWav2vec20Framework2020} conclude that pre-training speech recognition models on unlabeled data shows large potential, especially for ultra--low resource scenarios with very little labeled speech data available.
This could improve the availability of speech recognition systems for many lesser spoken languages as well as dialects, as this still remains a challenge in ASR (Section \ref{sec:challenges_in_asr}).
They conclude that self-supervised approaches also benefit from even more unlabeled data.

Whisper and wav2vec~2.0 thus need to be viewed as complementary approaches, rather than competing solutions.
They also cater to different needs: Whisper is a full end-to-end model that can be directly applied to different data sets, while wav2vec~2.0 needs careful fine-tuning for it to be accurate, since it doesn't have exhibit the same effective robustness as Whisper.



\begin{comment}
Models such as wav2vec~2.0, when fine-tuned on LibriSpeech, can achieve very low WER on in-distribution test sets but often degrade sharply on unseen datasets.
In contrast, zero-shot Whisper models exhibit more stable performance across distributions and more closely resemble human robustness patterns, even when their in-distribution WER is comparable.
Consequently, robustness evaluation complements standard benchmark testing by revealing differences in generalization behavior that are not captured by in-distribution WER alone.

wav2vec 2.0:
  can be adapted to streaming
whisper:
  only offline/non-streaming usage
    global attention
    not optimized for latency
  optimized for robustness (very important in asr) and accuracy
  multi-lingual
  multi-task
  actual end-to-end product

  WER calculation: first apply text normalization

general trends:
  more capable models (e.g. translation, multi-language in whisper) trained on even more (labeled) data
  self-supervised learning in (ultra-)low-resource scenarios?
  transformer has emerged as superior architecture
  more of the ASR pipeline built directly into models, such as inverse text normalization in whisper
  more joint learning e.g. of representations, positional encodings, attention
\end{comment}

\bibliography{zotero}

\end{document}
