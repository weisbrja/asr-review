\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{siunitx}
\usepackage{mathptmx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\title{A Review of Automatic Speech Recognition}

\author{Jannis Weisbrodt \\
  KIT \\
  \texttt{jannis.weisbrodt@student.kit.edu}
  }

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here.
\end{abstract}

\section{Introduction}

Automatic Speech Recognition (ASR) is the task of transcribing spoken language into writing -- one of the earliest goals of computer language processing \cite{jurafskySpeechLanguageProcessing2025}.
Today, automatic transcription is used ubiquitously not only in fields like law, where dictation is essential, but also in voice-controlled assistants as well as automatic captioning for movies, videos, conferences or voice messages.
Advances in ASR not only enhance accessibility for individuals with typing or hearing impairments, but also generally provide a more natural interface for interacting with modern machines.
Despite the progress, accurate automatic transcription of speech by any speaker in any environment still remains a challenging problem that is far from solved.

In this review, we will describe the ASR task and the challenges making it hard to achieve high accuracy in a lot of scenarios.
We will then talk in detail about the different approaches taken by two state-of-the-art ASR systems, comparing their paradigms and performance.
% historic development of performance metric

\section{Challenges in ASR}\label{sec:challenges_in_asr}

The ASR task varies along multiple dimensions, one of them being vocabulary size.
Some low vocabulary ASR tasks such as digit recognition or detecting \enquote{yes} versus \enquote{no} answers have been solved for quite a while.
Tasks that require accurate transcription of speech using an unrestricted large vocabulary, like videos or conversations, represent much harder challenges \cite{jurafskySpeechLanguageProcessing2025}.

A very important dimension is given by the environment and setup used for recording the audio data to be transcribed.
Audio recorded in a quiet studio environment without much noise close to a high quality microphone is much easier to transcribe than audio recorded on a busy street or inside a restaurant with the target standing far away from the recording microphone.

It also matters who the speaker is talking to.
Conversational speech between humans is much harder to transcribe accurately, than speech addressed directly towards a machine, as speakers tend to use a more simple register as well as articulating themselves more clearly in such scenarios.
The transcription of conversational speech is also generally made much harder due to a higher speaker count.

At last, varying speaker accents and styles add another dimension to the task of ASR.
This variation is heavily compounded by the large number of spoken languages, for many of which large amounts of labeled data simply do not exist \cite{baevskiWav2vec20Framework}.
Such low-resource scenarios still remain a hard challenge in ASR \cite{jurafskySpeechLanguageProcessing2025}.
Additionally, code-switching complicates things even further.

Spoken language also frequently contains disfluencies, such as fragmented, repeated or truncated words (e.g. \enquote{main- mainly}), as well as filled pauses (e.g. \enquote{uhm}, \enquote{uh}), differentiating ASR from other NLP tasks such as machine translation.

\section{Approaches to ASR}

Historically, there have been many approaches to the ASR task.
In this review, however, we focus only on two paradigms:
self-supervised encoder-only speech representation models fine-tuned with CTC loss as exemplified by wav2vec~2.0 \citep{baevskiWav2vec20Framework}, and weakly-supervised attention-based encoder-decoder architectures as exemplified by OpenAI's Whisper model \citep{radfordRobustSpeechRecognition2023}.

Before diving into the respective paradigms, we will first discuss some common techniques applied in many ASR systems.
These systems often times consist of multiple components, built around a core speech recognition model.
Most of the time, these components are arranged as a pipeline and can include audio format conversion, feature extraction, subsampling, voice activity detection and (inverse) text normalization \cite{radfordRobustSpeechRecognition2023}.

\subsection{Speech Feature Extraction}

The most basic representation of audio data, also referred to as the waveform, uses the amplitude of the recorded sound waves at each time frame.
These amplitudes are digitized by sampling and quantizing them at a given sampling frequency (e.g. $\SI{8}{\kilo\hertz}$ for telephone speech or $\SI{16}{\kilo\hertz}$ for microphone data).
Often times, this representation is not used directly as the input to the speech recognition model.
Instead, the system computes the log mel spectrum for small windows of time (characterized by the window size, stride and shape) and uses those feature vectors as the input.
The mel is a unit of pitch representing the human perception of changes in pitch (which is roughly linear for frequencies between $\SI{100}{\hertz}$ and $\SI{1000}{\hertz}$ and logarithmic for frequencies above this range) more accurately than raw frequency values.
By computing the frequencies present in the audio signal for each time window using a Fourier transform, we can sample the mel spectrum values for this time window by accumulating the energy levels of each frequency range using the mel filter bank.
Because human perception of amplitude is also logarithmic, we then take the log of each of the mel spectrum values, resulting in a vector of scalars, each of which is called a channel  \cite{jurafskySpeechLanguageProcessing2025}.
It is also common for ASR systems to then normalize these channels to a zero mean and unit variance.

\subsection{Text normalization}

Many ASR systems apply some form of text normalization to the transcriptions in the training data \cite{jurafskySpeechLanguageProcessing2025}.
This step is mostly rule based and might include stripping out punctuation marks, converting everything to upper case or standardizing spelling to the same language variant (e.g. US versus UK English).
If needed, inverse text normalization is then applied to the output of the speech recognition model.

\subsection{Attention-based Transformer Architecture}

% TODO: this section is very alone here and should probably be integrated somewhere else
\citet{vaswaniAttentionAllYou2023} showed that the Transformer architecture, which entirely replaces recurrent connections with multi-headed self-attention, achieves performance surpassing competitive recurrent models of its time while requiring substantially lower training cost.
This architecture also generalizes well to different sequence-to-sequence modeling tasks, such as ASR and has since provided the prevalent building blocks of state-of-the-art ASR systems.

\subsection{Self-Supervised Encoder-only Speech Representation Models}\label{sec:wav2vec2.0}

As discussed in Section~\ref{sec:challenges_in_asr}, a major challenge for modern neural networkâ€“based ASR systems is the lack of large labeled datasets containing thousands of hours of transcribed speech for most of the world's languages.
\citet{baevskiWav2vec20Framework} note, however, that human infants are able to acquire language understanding without such labeled examples anyway.
Instead, infants listen to adults around them speaking a language and gradually learn useful representations of speech.
\citet{baevskiWav2vec20Framework} argue, that self-supervised learning on unlabeled speech data can provide a similar mechanism for machine learning models.
Self-supervised learning has already proven effective across other NLP tasks and in other fields of research such as computer vision.
Importantly, with wav2vec 2.0, they focus solely on learning speech representations, since directly learning to transcribe from unlabeled data is infeasible.
The learned representations are then fine-tuned on labeled data using a Connectionist Temporal Classificiation (CTC) loss \cite{gravesConnectionistTemporalClassification} to perform tasks such as speech recognition.


% TODO: describe the architecture

% con: CTC makes strong independence assumption
% -> leads to what errors?
% balanced out by applying LLM how?


\subsection{Attention-based Encoder-Decoder Architecture}

% TODO: rewrite after writing section on wav2vec 2.0
% make it clear, that whisper improves on wav2vec 2.0, without the unsupervised techniques
Due to the unavailability of high-quality large labeled datasets for ASR in many languages, self-supervision techniques such as wav2vec 2.0 \cite{baevskiWav2vec20Framework} made use of the large amounts of readily available unlabeled speech data, achieving astounding success.
Separately, research in other fields such as computer vision showed that training on much larger weakly supervised datasets, rather than only high-quality labeled data, leads to significant improvements in robustness and generalization of models \cite{radfordRobustSpeechRecognition2023}.

% TODO: make clear, that it was a huge feat to construct these new large datasets
Motivated by these trends, \citet{radfordRobustSpeechRecognition2023} identified a key limitation of ASR models such as wav2vec~2.0, which only use a pre-trained encoder: whilst they learn powerful speech representations, they lack an equivalently strong pre-trained decoder and have to therefore rely heavily on fine-tuning to perform tasks such as speech recognition.
\citet{radfordRobustSpeechRecognition2023} instead pre-trained an entire encoder-decoder Transformer on large amounts ($680{,}000$ hours) of weakly supervised labeled speech data, successfully achieving a new state of the art with the Whisper models.
% no need for self-supervision, instead relying on more weakly-supervised data
% wav2vec 2.0 better for low-resource languages?

% whisper doesn't use (inverse) text normalization, except when filtering out bad transcripts


% supervised fine-tuning after unsupervised pre-training in wav2vec 2.0/weakly-supervised pre-training in whisper
% convolutions as preprocessing step
% pre-activation residual blocks by having layer norm applied first

% positional encoding

% in whisper:
% 1d convolutional layers compress input sequence and compute speech features

% talk about WER calculation by applying text normalization first

% general trends:
% more capable models (e.g. translation, multi-language in whisper) trained on even more (labeled) data
% unsupervised learning in (ultra-)low-resource scenarios?
% transformer has emerged as superior architecture
% more of the ASR pipeline built directly into models, such as inverse text normalization in whisper
% more joint learning e.g. of representations, positional encodings, attention

\bibliography{zotero}

\end{document}
