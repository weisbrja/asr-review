\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[english]{babel}
\hyphenation{OpenAI}
\hyphenation{LibriSpeech}
\hyphenation{LibriVox}

\usepackage{microtype}

\usepackage{siunitx}
\usepackage{mathptmx}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{comment}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{csquotes}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage[absolute,overlay]{textpos}
\usepackage{xcolor}
\usepackage{enumitem}

\title{A Review of Automatic Speech Recognition}

\author{Jannis Weisbrodt \\
  Karlsruhe Institute of Technology \\
  \texttt{jannis.weisbrodt@student.kit.edu}
  }

\begin{document}

\maketitle

\begin{abstract}
  Automatic speech recognition (ASR) transcribes spoken language into text, a task complicated by large vocabularies and variability in speech and recording conditions.
  This survey reviews the classical ASR pipeline and the shift toward attention-based models, and provides a comparative analysis of wav2vec~2.0 and Whisper as representative approaches to self-supervised representation learning and large-scale weakly supervised multilingual and multitask training.
  We examine standard evaluation methodology, focusing on Word Error Rate (WER) and robustness.
  Reported results for wav2vec~2.0 facilitate the possibility of low-resource ASR, while Whisper approaches human-level robustness, underscoring the benefits of training at scale as well as multilingual and multitask learning.
  We finish with open questions regarding robustness and future directions in representation learning.
\end{abstract}

\section{Introduction}
Automatic Speech Recognition (ASR) is the task of transcribing spoken language into writing---one of the earliest goals of computer language processing \cite{jurafskySpeechLanguageProcessing2025}---and represents a subfield within natural language processing (NLP).
Today, automatic transcription is used ubiquitously not only in fields like law, where dictation is essential, but also in voice-controlled assistants and as automatic captioning for movies, videos, conferences or voice messages.
Advances in ASR not only enhance accessibility for individuals with typing or hearing impairments, but also generally provide a more natural interface for interacting with modern machines.
Despite the progress, accurate automatic transcription of speech by \emph{any speaker} in \emph{any environment} in an \emph{open vocabulary} setting still remains a challenging problem that is far from solved.

In this review, we begin by introducing the ASR task and outlining the challenges involved in producing accurate transcriptions of speech.
We then examine how ASR systems typically process speech, covering the classical ASR pipeline and the shift toward attention-based speech recognition models.
Next, we focus on two representative ASR systems, describing and comparing their architectures, training procedures and inference processes, before discussing evaluation and their performance.
We conclude by summarizing the insights into modern computer-based speech recognition provided by these models, as well as outlining remaining open questions.

\section{Challenges in ASR}\label{sec:challenges_in_asr}

The complexity of the ASR task can be factored along multiple dimensions that each affect transcription accuracy.

\paragraph{Vocabulary Size}
One of the main challenges in ASR is the potentially very large vocabulary that needs to be recognized and accurately transcribed.

While some low vocabulary tasks such as digit recognition or detecting \enquote{yes} versus \enquote{no} answers have been effectively solved for some time, transcribing unconstrained speech with a large general vocabulary remains considerably more difficult \cite{jurafskySpeechLanguageProcessing2025}.

As vocabulary size increases, lexical ambiguity and the likelihood of rare or unseen words also grow, especially in multilingual or domain-specific settings (e.g., medical, legal or technical terminology).
Code-switching---switching between multiple languages within the same utterance---further increases this complexity.

\paragraph{Acoustic and Speaker Variability}
Beyond vocabulary size, ASR must account for the fact that the same sentence can be uttered in many acoustically distinct ways.
Differences in dialect or accent, speaking rate, prosody, and vocal tract characteristics lead to substantial variation in the speech signals produced by different speakers, even when the linguistic content is identical.
This increases acoustic ambiguity and inherently complicates reliable speech recognition.

Accents and dialects---especially those associated with lesser-spoken languages---pose a significant challenge for ASR systems.
Many such varieties lack large, labeled speech corpora and are consequently underrepresented during training.
This imbalance often leads to substantially higher error rates for affected speaker groups \cite{jurafskySpeechLanguageProcessing2025}.
Developing robust ASR therefore requires models that generalize effectively under data scarcity, a persistent challenge in speech modeling \cite{baevskiWav2vec20Framework2020}.

Apart from variability in acoustics across speakers, spoken language frequently contains disfluencies, such as fragmented, repeated or truncated words (e.g., \enquote{main- mainly}), as well as filled pauses (e.g., \enquote{uhm}, \enquote{uh}).
These speech specific phenomena deviate from canonical word forms and introduce additional uncertainty when transcribing.

\paragraph{Audio Quality and Recording Environment}

A further dimension of complexity arises from the recording conditions under which speech is captured.
Audio recorded in a quiet studio environment with a high-quality microphone placed close to the speaker is substantially easier to transcribe than speech recorded in noisy, everyday settings such as busy streets, restaurants or echoing rooms.

Background noise and competing speakers can mask and distort parts of the already complex speech signal.
Reverberation in enclosed spaces can blur sounds over time, making it harder to distinguish individual words or phonemes.
The quality and placement of the recording microphone, as well as compression or transmission artifacts, can further distort the signal.

All of these factors make speech recognition under real-world conditions significantly more difficult.

\section{Approaches to ASR}

Historically, a wide range of approaches have been developed and applied to the ASR task.

\paragraph{ASR Pipeline}
Speech recognition systems typically consist of multiple components, arranged in a pipeline built around a core speech recognition model.
These components can include audio format conversion, feature extraction, subsampling, voice activity detection and inverse text normalization \cite{radfordRobustSpeechRecognition2023}.

\paragraph{End-to-End ASR Systems}
More recent end-to-end ASR systems, such as Whisper \cite{radfordRobustSpeechRecognition2023}, combine many of the traditional pipeline steps into a single model, instead of splitting them into clearly separated components \cite{prabhavalkarEndtoEndSpeechRecognition2023}.

However, many of the ideas described below are still somehow used within these systems.

In this section, we will therefore provide an overview of the classic ASR pipeline and its three main components: speech feature extraction, the speech recognition model and inverse text normalization.
When discussing the speech recognition model itself, we focus on the use of neural network--based architectures, and in particular the recent dominance of Transformer-based models.

\subsection{Speech Feature Extraction}

ASR systems often have exact requirements on the format of the audio data they process.

\paragraph{Raw Waveform}
The most basic representation of audio data, used for example by wav2vec~2.0, is referred to as the raw waveform.
This format represents audio data simply by the amplitude of the recorded sound waves at each time frame.
These amplitudes are digitized by sampling them at a certain sampling frequency and quantizing them to a certain bit depth.

Since speech recognition systems typically require their audio input to have a fixed sampling frequency (e.g., $\SI{16}{\kilo\hertz}$ for wav2vec 2.0 and Whisper) and a fixed bit depth, recorded audio data often needs to be resampled and requantized before it can be processed \cite{jurafskySpeechLanguageProcessing2025}.

\paragraph{Log Mel Spectrum}
Other ASR models, such as Whisper \citep{radfordRobustSpeechRecognition2023}, operate on a different audio representation and therefore require an additional preprocessing step.
Instead of processing the raw waveform directly, the system first computes the so called log mel spectrum for small windows of time (characterized by the window size, stride and shape) and uses the resulting feature vectors as the input.

The mel is a unit of pitch representing the human perception of changes in pitch (which is roughly linear for frequencies between $\SI{100}{\hertz}$ and $\SI{1000}{\hertz}$ and logarithmic for frequencies above this range) more accurately than raw frequency values.
By computing the frequencies present in the audio signal for each time window using a Fourier transform, we can sample the mel spectrum values for this time window by accumulating the energy levels of each frequency range using a mel filter bank.
Because human perception of amplitude is also logarithmic, we then take the log of each of the mel spectrum values, resulting in a vector of scalars, each of which is called a channel \cite{jurafskySpeechLanguageProcessing2025}.

\paragraph{Normalization}
It is also common for modern ASR systems to normalize their audio input.
\citet{baevskiWav2vec20Framework2020} rescale the input of wav2vec~2.0 to have zero mean and unit variance, while \citet{radfordRobustSpeechRecognition2023} normalize the computed channels to lie within the range $[-1,1]$ and to also have approximately zero mean before processing them with Whisper.

\subsection{Speech Recognition Model}

Now that we have discussed the pre-processing of the input audio in the form of feature extraction, we turn to the main component of modern ASR pipelines:
a speech recognition model responsible for converting the extracted speech features into output tokens.

More formally, the speech recognition model solves the task of computing the conditional probability $p(\mathbf{y}\mid\mathbf{x})$ of a sequence of linguistic units $\mathbf{y}=(y_1,\ldots,y_m)$ given the extracted speech features $\mathbf{x}=(x_1,\ldots,x_n)$.
The probability distributions over the linguistic units (output tokens) can then be sampled to generate actual transcriptions of the input audio \cite{jurafskySpeechLanguageProcessing2025}.

\paragraph{Neural Network--Based Model Architectures}
As in many other machine learning domains---particularly in NLP---the model is typically implemented using neural networks.
Over time, various neural architectures have been applied, including feedforward, convolutional, and recurrent models.
In recent years, many state-of-the-art systems have increasingly adopted attention-based designs \cite{prabhavalkarEndtoEndSpeechRecognition2023}.

\paragraph{Transformer Architecture}
In particular, the Transformer architecture, proposed by \citet{vaswaniAttentionAllYou2023}, now forms the prevalent foundation used by many state-of-the-art ASR systems, reflecting a broader trend in machine learning.

The Transformer architecture uses a self-attention mechanism to compute context-aware representations for each element in the input sequence by combining information from all positions, enabling the model to capture dependencies across the entire input sequence.

\paragraph{Self-Attention}
Self-attention is computed by projecting each element of the input sequence into query, key and value vectors, and using the dot-product similarity between a query and all keys to produce attention scores, which are then normalized and used as weights to sum the corresponding value vectors.

\paragraph{Multi-Head Attention}
To capture multiple types of relationships in parallel, a single self-attention layer deploys multiple attention heads, where each head has its own projections for computing query, key and value vectors and performs the attention calculation independently; the outputs of all attention heads are then concatenated and projected into a single contextualized representation.

\paragraph{Transformer Blocks}
These multi-headed self-attention computations as well as an additional feedforward network are then combined into a single Transformer block, each of them residually adding context to each input vector.

\paragraph{Encoder-Decoder Architecture}
Stacking such blocks can be used to create an encoder that transforms each feature vector of the input sequence into a contextualized representation.

A decoder can be built in a similar way, however with self-attention being causal (left-to-right) instead.
The first decoder block is then given all previous output tokens as input and in every decoder block, a feature vector may only attend to previous positions.

\paragraph{Cross-Attention}
Additionally, through cross-attention, the decoder can attend to the entire sequence of feature vectors generated by the encoder by using the outputs of the previous decoder block as queries, while the keys and values come from the encoder output.

\paragraph{Advantages of Transformer-based Architectures}
The Transformer architecture outperformed earlier speech recognition models, including recurrent networks, primarily because it captures long-range temporal dependencies in the input audio more effectively while simultaneously enabling parallel computation across the input sequence.

This has since made it a prevalent building block in many state-of-the-art ASR systems, including wav2vec~2.0 and Whisper.

\subsection{Inverse Text Normalization}\label{sec:inverse_text_normalization}

After describing how the speech recognition model maps speech features to output tokens, we discuss a post-processing stage typically applied at the end of the ASR pipeline.

\paragraph{Text Normalization}
Historically, ASR systems were often trained to produce normalized transcriptions, removing punctuation, enforcing uniform casing, standardizing spelling, and verbalizing dates, numbers, and symbols (e.g., \enquote{twenty-five dollars} instead of \enquote{\$25}).

\paragraph{Inverse Text Normalization}
To produce naturalistic transcripts, these systems then often implemented an inverse mapping from the standardized transcriptions back to conventional written text, possibly applying a specific locale in the process.
This step is often accordingly called inverse text normalization.

\paragraph{End-to-End ASR Models}
Newer end-to-end spech recognition systems, such as Whisper \cite{radfordRobustSpeechRecognition2023}, instead directly learn the mapping between utterances and their textual transcripts, eliminating the need for explicit inverse text normalization, thereby simplifying the ASR pipeline.

\section{Comparison of ASR models}\label{sec:comparison_of_models}

Having outlined the classical ASR pipeline, where neural networks---particularly Transformer-based architectures---serve as the core component, we now examine two representative models that reflect distinct paradigms of contemporary Transformer-based ASR:
wav2vec~2.0 \citep{baevskiWav2vec20Framework2020}, a self-supervised encoder-only model fine-tuned with CTC, and OpenAI's Whisper \citep{radfordRobustSpeechRecognition2023}, a weakly-supervised encoder-decoder model.

\subsection{Motivations}

The motivations behind wav2vec~2.0 and Whisper differ, but both address a central challenge of neural network--based ASR:
the scarcity of large labeled datasets (Section~\ref{sec:challenges_in_asr}).

\paragraph{Self-Supervision for Low-Resource Scenarios}
\citet{baevskiWav2vec20Framework2020} argue that limited labeled data should not fundamentally constrain ASR accuracy.
Drawing an analogy to human language acquisition, they observe that infants learn from exposure to unlabeled speech, gradually forming useful representations without explicit supervision.
Accordingly, \citet{baevskiWav2vec20Framework2020} propose leveraging self-supervised learning on unlabeled speech data---often readily available even in low-resource scenarios common to lesser-spoken languages with very limited transcribed speech---as an effective training paradigm for ASR, building on its success across other NLP tasks, as well as computer vision.

\paragraph{Self-Supervised Encoder-Only Models}
They propose training an encoder-only model to learn contextualized representations of jointly learnt quantized speech units.
For downstream tasks such as speech recognition, they add a small layer onto the encoder and fine-tune this new model on possibly very little labeled data using a Connectionist Temporal Classificiation (CTC) \cite{gravesConnectionistTemporalClassification}.

\paragraph{Disadvantages of Encoder-Only Models}
By contrast, \citet{radfordRobustSpeechRecognition2023} highlight a general limitation of such self-supervised pre-training approaches: while they are able to train audio encoders providing high-quality representations of speech, they lack an equally strong pre-trained decoder, requiring fine-tuning on labeled data for downstream tasks such as ASR; a complex process introducing the risk of the model adepting to dataset-specific patterns and reducing its ability to generalize well to other datasets.
Encoder-only models also often benefit from the use of an additional language model together with the fine-tuned decoder.

\paragraph{Large-Scale Weak Supervision}
\citet{radfordRobustSpeechRecognition2023} instead suggest adopting a large-scale weakly-supervised strategy, moving beyond existing gold-standard labeled datasets and pre-training standard encoder-decoder models on these substantially larger collections of labeled speech data.
This weakly-supervised approach has already been shown to improve robustness and generalization in other domains, such as computer vision.
For this purpose, they constructed Whisper, an end-to-end ASR model trained on the largest labeled speech dataset of its time, with the goal of achieving robust transcription of multilingual speech in a zero-shot setting, meaning without any fine-tuning.

\subsection{Architectures}

After talking about the motivations behind the two models, we will now discuss their architectures.

\paragraph{Speech Feature Extraction}
The speech feature extraction in wav2vec~2.0 works by building latent speech feature representations directly from the raw input waveform.
The feature encoder consists of a stack of 1D convolutional layers each followed by layer normalization \cite{baLayerNormalization2016} and a GELU activation function \cite{hendrycksGaussianErrorLinear2023}.

Whisper also uses raw audio waveforms as input, however, it first computes a log mel spectogram of this waveform, normalizes the computed channel values and passes them to a small convolutional feature encoder, similar to the one just described used by \citet{baevskiWav2vec20Framework2020}.

\paragraph{Speech Recognition Model}
With wav2vec~2.0, the latent speech features computed by the feature encoder are then passed to the so called context network, consisting, for now, only of the encoder part of a Transformer \cite{vaswaniAttentionAllYou2023}.
This includes multiple blocks of residual connections, self-attention layers, feed-forward networks and layer normalization and serves the purpose of building contextualized representations of speech units, which can later be decoded into a textual transcription by appending an additional classification layer to the context network after pre-training.

The output of Whisper's feature encoder is instead passed to an entire encoder-decoder Transformer \cite{vaswaniAttentionAllYou2023}, using pre-activation residual connections and sharing the same width and number of Transformer blocks between encoder and decoder, which are connected through cross-attention.

\paragraph{Positional Embeddings}
\citet{baevskiWav2vec20Framework2020} use relative positional embeddings as the positional information used by the encoder.
They compute these embeddings by adding a convolution of the inputs followed by a GELU activation on to the inputs and then applying layer normalization.

\citet{radfordRobustSpeechRecognition2023} opted to use sinosoidal position embeddings for the encoder and learned position embeddings for the decoder instead.

\subsection{Training}\label{sec:training}

After discussing the architectures used by the two representative models, we will now contrast their respective training processes.

\paragraph{Training Paradigms}
As stated previously, \citet{baevskiWav2vec20Framework2020} employ a self-supervised pre-training approach for wav2vec~2.0, while \citet{radfordRobustSpeechRecognition2023} use a supervised approach for Whisper.
Additionally, Whisper's training process also includes learning language identification and translation
The training processes for the two models therefore differ very much.

\paragraph{Training Datasets}
Apart from the difference in paradigm, the approaches also differ in their training data.
For the self-supervised pre-training of wav2vec~2.0, \citet{baevskiWav2vec20Framework2020} use the LibriSpeech corpus \cite{panayotovLibrispeechASRCorpus2015}, containing $\SI{960}{\hour}$ of unlabeled data, as well as the larger LibriVox corpus \cite{kahnLibriLightBenchmarkASR2020}, amounting to $\SI{53.2}{\kilo\hour}$ of audio data.

Instead of using an existing corpus, \citet{radfordRobustSpeechRecognition2023} curate $\SI{680}{\kilo\hour}$ of labeled audio data from publicly accessible sources on the internet, consisting of $\SI{438}{\kilo\hour}$ of English speech, $\SI{117}{\kilo\hour}$ of multilingual speech and $\SI{125}{\kilo\hour}$ of multilingual speech with English transcriptions.
This amounts to one order of magnitude more labeled data than previous approaches.

\paragraph{Data Augmentation}
Both models employ (a modified version of) SpecAugment \citet{parkSpecAugmentSimpleData2019} to augment labeled training data, with wav2vec~2.0 using it for fine-tuning (Section~\ref{sec:fine-tuning}).

\paragraph{Self-Supervised Pre-Training using Masking}
The pre-training of wav2vec~2.0 works similar to the training of masked language models such as BERT \cite{devlinBERTPretrainingDeep2019}.

Given an unlabeled speech sample as input, random time spans of the audio are chosen and the corresponding speech feature vectors computed by the feature encoder are masked out.
Masked out feature vectors are simply replaced by a trained feature vector shared between all masked time steps.

The masked feature vectors are then passed to the context network, producing contextualized representations of speech for each time step.

\paragraph{Quantization and Training Objective}
A contrastive loss is then calculated for each masked time step:
A quantization module---only used during pre-training---computes quantized speech representations for each of the masked feature vectors by discretizing each unmasked feature to a finite set of speech representations using product quantization \cite{jegouProductQuantizationNearest2011}, an approximate nearest-neighbor method that maps each feature vector to a combination of learned codebook entries.

Each of the quantized representations are then used in a classification problem:
Given the contextualized representation for a certain masked time step, the model must identify the correct quantized speech representation associated with that time step from a set of candidate representations containing distractors.
The distractor representations for this contrastive task are quantized representations sampled from some of the other masked time steps within the same utterance, ensuring that the negative examples are acoustically and contextually plausible.

Rather than employing an explicit classification loss, the cosine similarity scores between the contextualized representation of the masked time step and each candidate representation are used to calculate the contrastive loss, which is then be used by the optimizer.
The contrastive training objective encourages the similarity score of the true target to be higher than those of the distractors, thereby pushing the model to align the contextual representations and the discrete speech units.

\paragraph{Jointly Learnt Quantizations}
\citet{baevskiWav2vec20Framework2020} also enable the gradients of the contrastive loss to flow back through the quantization module during optimization by using a hard selection for determining the quantized representations in a forward pass and assuming a probabilistic soft selection in the backwards pass of the pre-training step (Gumbel softmax).
As a result, the codebook embeddings used by the quantization module are learned jointly with the rest of the model, allowing the quantized speech units themselves to adapt to the structure of the speech data.

Additionally, \citet{baevskiWav2vec20Framework2020} employ a diversity loss with the purpose of preventing codebook entries from going unused in the quantization module.

\paragraph{Multiask Supervised Pre-Training}
The pre-training process of Whisper for speech recognition is conceptually simpler, as it uses a standard Transformer architecture and sequence-to-sequence cross-entropy loss.

In addition to generating transcripts, \citet{radfordRobustSpeechRecognition2023} also train Whisper to do voice activity detection and prediction of timestamps along the transcript.
Furthermore, for multilingual settings, the model is trained to perform language identification and to translate speech into English.

For a given transcribed speech sample, the audio data is first processed by the encoder, which produces a sequence of latent speech representations, similar to those produced by the context network present in wav2vec~2.0.

% TODO; describe whisper's multi-task training: translation, transcription for multiple languages, voice activity detection, speaker diarization, time stamps

% TODO: talk about how alignment is learned

For the multitask training, the decoder can then be given tokens corresponding to each task.

\paragraph{Autoregressive Decoding}
Assuming the task of transcription, the decoder then predicts the transcript autoregressively, generating one token at a time.
During training, each prediction of the next transcript token is conditioned on the previously correct tokens, with a small probability of using previously generated tokens instead.

To produce the next token, the decoder applies self-attention over the preceding transcript tokens and later on over the latent feature sequence, as well as cross-attention to the encoded audio features produced by the encoder.

This mechanism closely mirrors the inference procedure, which is described in more detail in Section~\ref{sec:inference}.

\paragraph{Training Objective}
The optimizer then minimizes the cross-entropy loss over the generated token.
This is done for all expected transcript tokens.

\subsection{Fine-Tuning}\label{sec:fine-tuning}

After pre-training a speech recognition model, an additional step of fine-tuning on a testset can be applied.

\paragraph{Adapting Encoder-Only Models for ASR}
For encoder-only models, such as wav2vec~2.0, a slight modification of the model is required to perform downstream ASR, because pre-training only teaches the model to produce contextualized speech representations rather than actual transcript text.

To enable transcription, a randomly initialized linear projection layer is appended to the context network of the pre-trained encoder-only model.
This added layer maps each contextualized representation to vector of logits over the output vocabulary, such as characters or phonemes, including a special CTC boundary symbol.
A softmax operation is then applied to obtain a probability distribution over these tokens.

Because this classification layer is introduced after pre-training, its parameters must be learned later, making fine-tuning non-optional.

% TODO: talk about what labeled data wav2vec 2.0 is fine-tuned on

\paragraph{Fine-Tuning of Supervised Models}
In contrast, supervised models are already trained on large collections of labeled speech data and are therefore capable of performing ASR without additional fine-tuning.
Nevertheless, fine-tuning remains common practice, particularly when adapting a model to a specific domain or acoustic conditions.

For Whisper specifically, \citet{radfordRobustSpeechRecognition2023} design the model to perform well without task-specific adaptation, aiming for strong zero-shot performance across a wide range of datasets and conditions.

\paragraph{Fine-Tuning using CTC}
The actual fine-tuning of wav2vec~2.0 performed by \citet{baevskiWav2vec20Framework2020} uses CTC loss \cite{gravesConnectionistTemporalClassification} with some (possibly very little) labeled speech data.
The learned feature encoder stays fixed and is not trained during this process.
Latent speech features produced by it are however tampered with during fine-tuning using a method similar to SpecAugment \cite{parkSpecAugmentSimpleData2019}, as previously stated, because this improves generalization and robustness.

Fine-tuning the decoder using CTC loss has the goal of maximizing the probability of the model outputting the correct transcript, when all duplicate output tokens, e.g.\ characters or phonemes, between CTC boundary tokens are collapsed.
This collapse is also applied during inference (Section~\ref{sec:inference}).

\paragraph{Marginalization over Alignments}
Because different alignments of the input audio and the output tokens can amount to the same transcript being generated, the CTC loss marginalizes over all valid alignments of output tokens---the sequences of output tokens that collapse to the ground-truth transcript.
This marginalization is efficiently computed using dynamic programming, summing the probabilities of all such alignments without explicitly enumerating and calculating the probability of each alignment.
The dynamic programming approach used here takes advantage of the fact that alignments producing the same prefix of the target transcription at the same time step in the input audio share the same accumulated probability up to that point.

\paragraph{Conditional Independence Assumption}
One of the main issues in training a decoder using CTC loss, as done in wav2vec~2.0, is that CTC relies on a strong conditional independence assumption.
Every output token $y_i$ in the output sequence $\mathbf{y}=(y_1, \ldots, y_m)$ is assumed to be conditionally independent of every other output token $y_j$ given the input sequence $\mathbf{x}$ of audio speech features:
\begin{equation*}
  p(\mathbf{y}\mid\mathbf{x}) = \prod_{t=1}^m p(y_t\mid\mathbf{x})
\end{equation*}

\paragraph{Implicit Language Modeling}
As a result, the layer added onto a pre-trained wav2vec~2.0 model cannot capture sequential dependencies between output tokens and does therefore not learn an implicit language model, unlike Whisper, which does not make this assumption of conditional independence.

\paragraph{Explicit Language Modeling}
Not learning an implicit language model can partially be overcome by including an external language model when decoding (Section~\ref{sec:inference}).
It is even possible for this approach to prove beneficial, for example when transferring between different ASR domains:
even though the language used in the domains might differ, the acoustics of speech often still remain similar.
With a decoder trained only using CTC, a new language model can then simply be swapped in to adapt to the new domain \cite{jurafskySpeechLanguageProcessing2025}.

\subsection{Inference}\label{sec:inference}

Having discussed the different training paradigms and fine-tuning procedures, we now consider inference.

% say that both models have this basic fact in common:
Both Whisper and wav2vec~2.0 share the same fundamental principle at inference time:
the latent representations produced by their respective encoders are ultimately converted into output tokens.

\paragraph{Output Tokens}
Whisper's decoder generates BPE tokens \cite{sennrichNeuralMachineTranslation2016, radfordLanguageModelsAre2019}, necessitating a mapping step to achieve a human readable transcript, while wav2vec~2.0 already generates characters (or phonemes) but still requires the CTC collapse described in Section~\ref{sec:fine-tuning} to generate the final transcript.

\paragraph{Inference under Conditional Independence}
The biggest difference in inference between wav2vec~2.0 and Whisper is due to the conditional independence assumption made only by the former, but not the latter.

Because \citet{baevskiWav2vec20Framework2020} assume output tokens to only depend on the latent representations generated by the encoder and be independent of each other, inference of an output token for each time step of the input audio can be done in one pass.
The CTC collapse then only requires a further linear pass over the generated output tokens.

% TODO: add source
\paragraph{Autoregressive Decoding}
For Whisper, inference happens autoregressively instead.
When generating a new output token, the decoder not only attends to the latent representations of the encoder through cross-attention, but also to all previously generated output tokens---each using a learned positional encoding---using causal (left-to-right) self-attention.
This autoregessive property causes inference to be inherently sequential, making it computationally less efficient than the inference for wav2vec~2.0.

\paragraph{Decoding Strategies}
Because the decoders of both Whisper and wav2vec~2.0 output a probability distribution over the possible output tokens rather than a single concrete token, this distribution must then be converted into an actual prediction via a decoding strategy.
This can be done, for example, by greedy decoding, which selects the most probable token, or by beam search.

% TODO: add source
\paragraph{Beam Search}
Beam search works by keeping the most probable candidate sequences of output tokens in a search beam.
At each step, every sequence in the beam is extended by all possible next output tokens, the probabilities of the resulting sequences are calculated by accumulating output token probabilities computed by the model, and then only the most probable sequences are retained for the next step.

This decoding strategy incurs additional computational cost compared to greedy decoding, but it also enables the incorporation of an external language model in the decoding process.

% TODO: add source
\paragraph{External Language Model Rescoring}
The inclusion of an external language model can be achieved by rescoring each candidate sequence in the beam, interpolating the probability assigned by the external language model with the probability computed by the decoder using a certain weighting factor.
Additionally, since language models tend to prefer shorter sequences, a length normalization term is also included in the new score.

% TODO: add references to evaluation of incorporating external language models for inference
Since wav2vec~2.0 does not learn an implicit language model (Section~\ref{sec:fine-tuning}), integrating such an external language model (at least for transcript generation) is particularly beneficial \cite{baevskiWav2vec20Framework2020}.
For Whisper, the usage of external language models can still improve transcript quality, as its implicit language model is trained on substantially less text data than modern LLMs \cite{jurafskySpeechLanguageProcessing2025}.

\subsection{Evaluation}\label{sec:evaluation}

After describing the architecture, training, and inference procedures of wav2vec~2.0 and Whisper, we now turn to methods for assessing and comparing the transcription accuracy of ASR systems.

\paragraph{Test Dataset}
Typically, ASR performance is evaluated on a held-out test set, drawn from the same distribution as the training data, using standardized evaluation metrics to ensure consistency across studies.

\paragraph{Word Error Rate}
The most widely used evaluation metric for ASR is the Word Error Rate (WER).
It quantifies the discrepancy between a generated transcription and a reference transcript by computing the minimum word-level edit distance between the two sequences, normalized by the total number of words in the reference, denoted by $N$.

\paragraph{Edit Distance}
The minimum edit distance of two sequences is defined as the minimum number of substitutions $S$, deletions $D$ and insertions $I$ required to transform one sequence into the other.

This minimum can be efficiently computed via a dynamic programming approach that incrementally determines the edit distance for all prefix pairs of the two sequences.

WER is therefore given by
\begin{equation*}
  \text{WER} = \frac{S+D+I}{N}.
\end{equation*}

\paragraph{Advantages of WER}
WER is popular in speech processing due to its simplicity while being largely language-agnostic---though for some languages, alternatives such as character-level metrics may be more appropriate.
Its straightforward computation ensures consistency and reproducibility across studies, providing a clear and easily interpretable measure of transcription accuracy, with lower values indicating fewer errors.

\paragraph{Limitations of WER}
Despite its usefulness, WER has several notable limitations.
First of all, it ignores semantic context, treating all word errors equally regardless of their impact on the overall meaning of the transcription.
This causes small errors such as transcribing \enquote{piece} instead of \enquote{pieced} to be penalized the same as transcribing an entirely different word.

WER is also highly sensitive to variations in transcript style, such as punctuation or spelling variations, which can lead to artificially higher error scores when the style of the reference transcriptions in the test set differs from the style implicitly learned by the model.

\paragraph{Fine-Tuning before Evaluation}
To circumvent the latter problem, ASR models are often fine-tuned on a development test set, which contains transcriptions similar in style to those found in the actual test set.
While fine-tuning on such a development set improves comparability of model performance for each specific test set, measuring the performance of models after adapting them to the transcription style of each test set inherently changes what is being measured by the evaluation.

\paragraph{In- vs.\ Out-of-Distribution Evaluation}
This is because evaluating the WER of models that are fine-tuned to data sampled from the same distribution as the test set primarily reflects their ability of in-distribution generalization.

\citet{radfordRobustSpeechRecognition2023} argue that this setting differs fundamentally from how human performance is typically evaluated on the same speech recognition tasks, since humans are often given little to no data from the test distribution prior to evaluation.
Their performance is instead assessed in many different settings without prior fine-tuning.
This measures their out-of-distribution generalization, meaning their ability to maintain a low WER across different testing conditions, which in the context of ASR systems is commonly referred to as robustness.

When evaluating ASR systems that match or even outperform humans on individual benchmarks in a similar manner across different test sets, \citet{radfordRobustSpeechRecognition2023} observed large drops in model performance, highlighting a lack of robustness.

\paragraph{Evaluating Robustness}
To capture the so called overall robustness of an ASR system, \citet{radfordRobustSpeechRecognition2023} measure the average WER of models across a suite of heterogeneous test sets.

Because this overall robustness is directly influenced by a higher base accuracy, they also measure effective robustness, as introduced by \citet{taoriMeasuringRobustnessNatural2020}, which instead reflects out-of-distribution performance relative to what would be expected based on in-distribution accuracy.
A model with high effective robustness exhibits degradation in performance smaller than expected and comes closer to the ideal of equal performance across all datasets.

\paragraph{LibriSpeech Benchmark}
In practice, many ASR systems are evaluated on the LibriSpeech benchmark \cite{panayotovLibrispeechASRCorpus2015}, which \citet{radfordRobustSpeechRecognition2023} therefore use as an anchor for in-distribution performance, with performance on additional test sets evaluated relative to this baseline.

\paragraph{Zero-Shot Evaluation and Text Normalization}
To solve the issue of an inflated WER because of small stylistic mismatches in transcriptions without relying on fine-tuning, \citet{radfordRobustSpeechRecognition2023} use a zero-shot approach and instead apply dataset-specific text normalization (Section~\ref{sec:inverse_text_normalization}) before calculating WER.
This produces semantically more accurate scores at the cost of reducing the direct comparability of WER accross different studies.

\subsection{Results}

Now that we have discussed how ASR systems can be evaluated in general, as well as discussing the advantages and limitations of WER, and the robustness measure used by \citet{radfordRobustSpeechRecognition2023}, we will see how these metrics apply to wav2vec~2.0 and Whisper models and compare the results.

\paragraph{Model Configurations}
As is common in contemporary machine learning research, both wav2vec~2.0 and Whisper are evaluated using multiple model configurations.

\citet{baevskiWav2vec20Framework2020} trained both a \textsc{Base} model of wav2vec~2.0 with 95 million parameters and a \textsc{Large} model with 317 million parameters.

After pre-training these models on either the $\SI{960}{\hour}$ of LibriSpeech audio or the roughly $\SI{60}{\kilo\hour}$ of audio data from LibriVox as described in Section~\ref{sec:training}, \citet{baevskiWav2vec20Framework2020} then consider five datasets for fine-tuning each model.

They simulate low-resource labeled data scenarios using test sets from Libri-light \cite{kahnLibriLightBenchmarkASR2020}, with labeled data ranging from as little as $\SI{10}{\minute}$, $\SI{1}{\hour}$ and $\SI{10}{\hour}$ up to $\SI{100}{\hour}$, as well as fine-tuning on the full LibriSpeech corpus ($\SI{960}{\hour}$ of labeled data).

They also investigate the usage of external language models (Section~\ref{sec:fine-tuning}) by pairing each fine-tuned model with some combination of no language model, a 4-gram language model or a Transformer-based language model, with both language models being trained on the LibriSpeech text corpus.

In contrast, \citet{radfordRobustSpeechRecognition2023} evaluate Whisper across a spectrum of model scales, ranging from 39 million parameters (\textsc{Tiny}) through \textsc{Base}, \textsc{Small}, and \textsc{Medium}, up to the \textsc{Large} configuration with roughly 1.55 billion parameters, each trained on the full weakly supervised corpus of $\SI{680}{\kilo\hour}$ of multilingual and multitask speech samples.

\paragraph{Self-Supervision for Low-Resource Scenarios}
\citet{baevskiWav2vec20Framework2020} find that the \textsc{Large} configuration of wav2vec~2.0 trained on the full $\SI{60}{\kilo\hour}$ of unlabeled audio data from LibriVox paired with the Transformer-based language model only needs fine-tuning on $\SI{10}{\minute}$ of labeled data to achieve a WER of $5.2/8.6$ on LibriSpeech's clean/other test sets, with the former containing primarily high-quality, low-noise recordings with clear pronunciation, while the latter includes more challenging conditions such as background noise, reverberation and accented speech, which make speech recognition substantially more difficult (Section~\ref{sec:challenges_in_asr}).

These results significantly improve on previous self-supervised methods and show that accurate ASR is achievable even in ultra-low-resource settings, potentially benefiting many lesser-spoken languages and dialects.

\paragraph{Scaling Laws for Self-Supervision}
Comparing performance across model configurations, \citet{baevskiWav2vec20Framework2020} find that increasing the amount of unlabeled audio data ($\SI{960}{\hour}$ vs. $\SI{60}{\kilo\hour}$) results in large improvements in performance, demonstrating a general scaling law for self-supervised approaches.

Increasing the model size from \textsc{Base} to \textsc{Large} also improves the WER in all tested scenarios.

\paragraph{Self-Supervision for High-Resource Scenarios}
wav2vec~2.0 also proves effective for high-resource scenarios, as the \textsc{Large} model pre-trained on the full LibriVox corpus ($\SI{60}{\kilo\hour}$ of unlabeled audio) and fine-tuned on the full $\SI{960}{\hour}$ of labeled audio from the LibriSpeech corpus achieves a WER of $1.8/3.3$ when paired with the Transformer-based language model.
This highlights the value of high-quality speech representations and demonstrates the competitiveness of self-supervised learning, even when substantial labeled data is available.

\paragraph{Distribution Shift and Robustness}
With respect to robustness, \citet{radfordRobustSpeechRecognition2023} showed that even though wav2vec~2.0 models perform well on LibriSpeech, applying them to different data distributions by directly transferring fine-tuned models to other test sets leads to a significant increase in WER, therefore demonstrating low effective robustness under distribution shift.

With Whisper, even though the best zero-shot Whisper model achieves a significantly higher WER of $2.5/5.2$ on LibriSpeech, a very different robustness across different test sets is achieved, with a reduction in WER by $55.2\%$ on average per test set compared to the WER of a wav2vec~2.0 model that performs within $0.1\%$ on LibriSpeech.

\paragraph{Robustness at Scale}
\citet{radfordRobustSpeechRecognition2023} find that training on the full $\SI{680}{\kilo\hour}$ of semi-labeled audio data they collected yields the most robust and best-performing Whisper model, with increases in the amount of training data strongly correlating with improved performance and robustness.
This observation aligns with the positive scaling behavior reported by \citet{baevskiWav2vec20Framework2020}.

However, \citet{radfordRobustSpeechRecognition2023} begin to observe diminishing returns when increasing dataset size, which they attribute to saturation effects as performance approaches human-level accuracy.

Scaling model capacity yields further performance gains, consistent with the trends reported by \citet{baevskiWav2vec20Framework2020}.
For English speech recognition in particular, \citet{radfordRobustSpeechRecognition2023} again observe diminishing returns, which they similarly explain as a consequence of nearing human-level performance.

\paragraph{Multitask and Multilingual Speech Recognition}
Further, \citet{radfordRobustSpeechRecognition2023} note that although jointly training a single model to perform multiple tasks across many languages might seem counterproductive because of interference in learning, at sufficient scale these effects do not dominate.
Instead, they find multitask and multilingual Whisper configurations to scale better and even outperform similar configurations trained on English-only data.

\section{Conclusion}

This survey has explored contemporary ASR by outlining its central challenges and comparing wav2vec~2.0 and Whisper as representative models of two influential paradigms: self-supervised representation learning and large-scale supervised training.

On one hand, we have shown that high-quality speech encoders pre-trained on large amounts of unlabeled audio can substantially reduce labeled data requirements, while remaining competitive in high-resource settings.
This makes ASR feasible even in ultra-low-resource scenarios, possibly enabling speech recognition for lesser-spoken languages and dialects.

On the other hand, large-scale supervised training on diverse, multilingual, and multitask data enables zero-shot speech recognition with robustness approaching human-level performance, underscoring the effectiveness of scaling both data and model capacity.

In conclusion, wav2vec~2.0 and Whisper are best viewed as complementary rather than competing approaches:
the former prioritizes labeled data efficiency through representation learning, whereas the latter emphasizes strong multilingual and multitask performance through large-scale end-to-end training.

Open questions remain regarding how to improve robustness without relying solely on data scaling---particularly for low-resource scenarios---possibly by more effectively combining strong speech encoders with more expressive decoders or language models.

\bibliography{zotero}

\end{document}
