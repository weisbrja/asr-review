\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{siunitx}
\usepackage{mathptmx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\title{A Review of Automatic Speech Recognition}

\author{Jannis Weisbrodt \\
  KIT \\
  \texttt{jannis.weisbrodt@student.kit.edu}
  }

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here.
\end{abstract}

\section{Introduction}

Automatic Speech Recognition (ASR) is the task of transcribing spoken language into writing -- one of the earliest goals of computer language processing \cite{jurafskySpeechLanguageProcessing2025}.
Today, automatic transcription is used ubiquitously not only in fields like law, where dictation is essential, but also in voice-controlled assistants as well as automatic captioning for movies, videos, conferences or voice messages.
Advances in ASR not only enhance accessibility for individuals with typing or hearing impairments, but also generally provide a more natural interface for interacting with modern machines.
Despite the progress, accurate automatic transcription of speech by any speaker in any environment still remains a challenging problem that is far from solved.

In this review, we will describe the ASR task and the challenges making it hard to achieve high accuracy in a lot of scenarios.
We will then talk in detail about some state-of-the-art ASR systems, comparing their architecture and performance.
% historic development of performance metric

\section{Challenges in ASR}

The ASR task varies along multiple dimensions, one of them being vocabulary size.
Some low vocabulary ASR tasks such as digit recognition or detecting \enquote{yes} versus \enquote{no} answers have been solved for quite a while.
Tasks that require accurate transcription of speech using an unrestricted large vocabulary, like videos or conversations, represent much harder challenges \cite{jurafskySpeechLanguageProcessing2025}.

A very important dimension is given by the environment and setup used for recording the audio data to be transcribed.
Audio recorded in a quiet studio environment without much noise close to a high quality microphone is much easier to transcribe than audio recorded on a busy street or inside a restaurant with the target standing far away from the recording microphone.

It also matters who the speaker is talking to.
Conversational speech between humans is much harder to transcribe accurately, than speech addressed directly towards a machine, as speakers tend to use a more simple register as well as articulating themselves more clearly in such scenarios.
The transcription of conversational speech is also generally made much harder due to a higher speaker count.

At last, varying speaker accents and styles add another dimension to the task of ASR.
This variation is heavily compounded by the large number of spoken languages, for many of which large amounts of labeled data simply do not exist \cite{baevskiWav2vec20Framework}.
Such low-resource scenarios still remain a hard challenge in ASR \cite{jurafskySpeechLanguageProcessing2025}.
Additionally, code-switching complicates things even further.

Spoken language also frequently contains disfluencies, such as fragmented, repeated or truncated words (e.g. \enquote{main- mainly}), as well as filled pauses (e.g. \enquote{uhm}, \enquote{uh}), differentiating ASR from other NLP tasks such as machine translation.

\section{Approaches to ASR}

Historically, there have been many approaches to the ASR task.
In this review, however, we focus only on two paradigms:
self-supervised encoder-only speech representation models fine-tuned with CTC loss as exemplified by wav2vec~2.0 \citep{baevskiWav2vec20Framework}, and weakly-supervised attention-based encoder-decoder architectures as exemplified by OpenAI's Whisper model \citep{radfordRobustSpeechRecognition2023}.

Before diving into the respective paradigms, we will first discuss some common techniques applied in many ASR systems.
These systems often times consist of multiple components, built around a core speech recognition model.
Most of the time, these components are arranged as a pipeline and can include audio format conversion, feature extraction, subsampling, voice activity detection and (inverse) text normalization \cite{radfordRobustSpeechRecognition2023}.

\subsection{Speech Feature Extraction}

The most basic representation of audio data, also referred to as the waveform, uses the amplitude of the recorded sound waves at each time frame.
These amplitudes are digitized by sampling and quantizing them at a given sampling frequency (e.g. $\SI{8}{\kilo\hertz}$ for telephone speech or $\SI{16}{\kilo\hertz}$ for microphone data).
Often times, this representation is not used directly as the input to the speech recognition model.
Instead, the system computes the log mel spectrum for small windows of time (characterized by the window size, stride and shape) and uses those feature vectors as the input.
The mel is a unit of pitch representing the human perception of changes in pitch (which is roughly linear for frequencies between $\SI{100}{\hertz}$ and $\SI{1000}{\hertz}$ and logarithmic for frequencies above this range) more accurately than raw frequency values.
By computing the frequencies present in the audio signal for each time window using a Fourier transform, we can sample the mel spectrum values for this time window by accumulating the energy levels of each frequency range using the mel filter bank.
Because human perception of amplitude is also logarithmic, we then take the log of each of the mel spectrum values, resulting in a vector of scalars, each of which is called a channel  \cite{jurafskySpeechLanguageProcessing2025}.
It is also common for ASR systems to then normalize these channels to a zero mean and unit variance.

\subsection{Text normalization}

Many ASR systems apply some form of text normalization to the transcriptions in the training data \cite{jurafskySpeechLanguageProcessing2025}.
This step is mostly rule based and might include stripping out punctuation marks, converting everything to upper case or standardizing spelling to the same language variant (e.g. US versus UK English).
If needed, inverse text normalization is then applied to the output of the speech recognition model.

\subsection{Self-Supervised Encoder-only Speech Representation Models}\label{wav2vec2.0}



\subsection{Attention-based Encoder-Decoder Architecture}

\citet{vaswaniAttentionAllYou2023} showed that the Transformer architecture, which entirely replaces recurrent connections (and convolutions) with multi-headed self-attention, achieves performance surpassing competitive recurrent models of its time while requiring substantially lower training cost.
This architecture also generalizes well to different sequence-to-sequence modelling tasks, such as ASR.

Subsequent advances in ASR were driven, for example, by self-supervised encoder pre-training, benefiting from the large amount of readily available unlabeled speech data, as described in section~\ref{wav2vec2.0}.
Seperately, research in other fields such as computer vision showed that training on much larger weakly supervised datasets, rather than only high-quality labeled data, leads to significant improvements in robustness and generalization of models \cite{radfordRobustSpeechRecognition2023}.

Motivated by these trends, \citet{radfordRobustSpeechRecognition2023} identified a key limitation of ASR models such as wav2vec~2.0, which only use a pre-trained encoder: whilst they learn powerful speech representations, they lack an equivalently strong pre-trained decoder and have to therefore rely heavily on fine-tuning to perfom tasks such as speech recognition.
\citet{radfordRobustSpeechRecognition2023} instead pre-trained an entire encoder-decoder Transformer on large amounts ($680{,}000$ hours) of weakly supervised labled speech data, successfully achieving a new state of the art with the Whisper models.

% what is done in a supervised way after unsupervised pre-training in wav2vec 2.0/weakly-supervised pre-training in whisper? -> fine-tuning

% explain general architecture

% explain what whisper does differently for asr:
% * convolutions as preprocessing step instead of log mel spectrum as input
% * large data set, inspired by wav2vec 2.0

% positional encoding

% in whisper:
% raw audio wave form as input
% 1d convolutional layers compress input sequence and compute speech features

\bibliography{zotero}

\end{document}
