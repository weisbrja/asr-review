\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage[english]{babel}
\usepackage{siunitx}
\usepackage{mathptmx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{csquotes}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\title{A Review of Automatic Speech Recognition}

\author{Jannis Weisbrodt \\
  KIT \\
  \texttt{jannis.weisbrodt@student.kit.edu}
  }

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here.
\end{abstract}

\section{Introduction}

Automatic Speech Recognition (ASR) is the task of transcribing spoken language into writing -- one of the earliest goals of computer language processing \cite{jurafskySpeechLanguageProcessing2025}.
Today, automatic transcription is used ubiquitously not only in fields like law, where dictation is essential, but also in voice-controlled assistants as well as automatic captioning for movies, videos, conferences or voice messages.
Advances in ASR not only enhance accessibility for individuals with typing or hearing impairments, but also generally provide a more natural interface for interacting with modern machines.
Despite the progress, accurate automatic transcription of speech by any speaker in any environment still remains a challenging problem that is far from solved.

In this review, we will describe the ASR task and the challenges making it hard to achieve high accuracy in a lot of scenarios.
We will then talk in detail about the different approaches taken by two state-of-the-art ASR systems, comparing their paradigms and performance.
% historic development of performance metric

\section{Challenges in ASR}\label{sec:challenges_in_asr}

The ASR task varies along multiple dimensions, one of them being vocabulary size.
Some low vocabulary ASR tasks such as digit recognition or detecting \enquote{yes} versus \enquote{no} answers have been solved for quite a while.
Tasks that require accurate transcription of speech using an unrestricted large vocabulary, like videos or conversations, represent much harder challenges \cite{jurafskySpeechLanguageProcessing2025}.

A very important dimension is given by the environment and setup used for recording the audio data to be transcribed.
Audio recorded in a quiet studio environment without much noise close to a high quality microphone is much easier to transcribe than audio recorded on a busy street or inside a restaurant with the target standing far away from the recording microphone.

It also matters who the speaker is talking to.
Conversational speech between humans is much harder to transcribe accurately, than speech addressed directly towards a machine, as speakers tend to use a more simple register as well as articulating themselves more clearly in such scenarios.
The transcription of conversational speech is also generally made much harder due to a higher speaker count.

At last, varying speaker accents and styles add another dimension to the task of ASR.
This variation is heavily compounded by the large number of spoken languages, for many of which large amounts of labeled data simply do not exist \cite{baevskiWav2vec20Framework}.
Such low-resource scenarios still remain a hard challenge in ASR \cite{jurafskySpeechLanguageProcessing2025}.
Additionally, code-switching complicates things even further.

Spoken language also frequently contains disfluencies, such as fragmented, repeated or truncated words (e.g. \enquote{main- mainly}), as well as filled pauses (e.g. \enquote{uhm}, \enquote{uh}), differentiating ASR from other NLP tasks such as machine translation.

\section{Approaches to ASR}

Historically, there have been many approaches to the ASR task.
In this review, however, we focus only on two paradigms:
self-supervised encoder-only speech representation models fine-tuned with CTC loss as exemplified by wav2vec~2.0 \citep{baevskiWav2vec20Framework}, and weakly-supervised attention-based encoder-decoder architectures as exemplified by OpenAI's Whisper model \citep{radfordRobustSpeechRecognition2023}.

Before diving into the respective paradigms, we will first discuss some common techniques applied in many ASR systems.
These systems often times consist of multiple components, built around a core speech recognition model.
Most of the time, these components are arranged as a pipeline and can include audio format conversion, feature extraction, subsampling, voice activity detection and (inverse) text normalization \cite{radfordRobustSpeechRecognition2023}.

\subsection{Speech Feature Extraction}

The most basic representation of audio data, also referred to as the waveform, uses the amplitude of the recorded sound waves at each time frame.
These amplitudes are digitized by sampling and quantizing them at a given sampling frequency (e.g. $\SI{8}{\kilo\hertz}$ for telephone speech or $\SI{16}{\kilo\hertz}$ for microphone data).
Often times, this representation is not used directly as the input to the speech recognition model.
Instead, the system computes the log mel spectrum for small windows of time (characterized by the window size, stride and shape) and uses those feature vectors as the input.
The mel is a unit of pitch representing the human perception of changes in pitch (which is roughly linear for frequencies between $\SI{100}{\hertz}$ and $\SI{1000}{\hertz}$ and logarithmic for frequencies above this range) more accurately than raw frequency values.
By computing the frequencies present in the audio signal for each time window using a Fourier transform, we can sample the mel spectrum values for this time window by accumulating the energy levels of each frequency range using the mel filter bank.
Because human perception of amplitude is also logarithmic, we then take the log of each of the mel spectrum values, resulting in a vector of scalars, each of which is called a channel  \cite{jurafskySpeechLanguageProcessing2025}.
It is also common for ASR systems to then normalize these channels to a zero mean and unit variance.

\subsection{Text normalization}

Many ASR systems apply some form of text normalization to the transcriptions in the training data \cite{jurafskySpeechLanguageProcessing2025}.
This step is mostly rule based and might include stripping out punctuation marks, converting everything to upper case or standardizing spelling to the same language variant (e.g. US versus UK English).
If needed, inverse text normalization is then applied to the output of the speech recognition model.

\subsection{Attention-based Transformer Architecture}

% TODO: this section is very alone here and should probably be integrated somewhere else
\citet{vaswaniAttentionAllYou2023} showed that the Transformer architecture, which entirely replaces recurrent connections with multi-headed self-attention, achieves performance surpassing competitive recurrent models of its time while requiring substantially lower training cost.
This architecture also generalizes well to different sequence-to-sequence modeling tasks, such as ASR and has since provided the prevalent building blocks of state-of-the-art ASR systems.

\subsection{Self-Supervised Encoder-only Speech Representation Models}\label{sec:wav2vec2.0}

\subsubsection{Motivation}

As discussed in Section~\ref{sec:challenges_in_asr}, a major challenge for modern neural networkâ€“based ASR systems is the lack of large labeled datasets containing thousands of hours of transcribed speech for most of the world's languages.

\citet{baevskiWav2vec20Framework} note, however, that human infants are indeed able to acquire language understanding without such labeled examples anyway.
Instead, infants listen to adults around them speaking a language and gradually learn useful representations of speech.
Further, \citet{baevskiWav2vec20Framework} argue that self-supervised learning on unlabeled speech data can provide a similar mechanism for machine learning models, since it has proven itself effective across many NLP tasks and in other fields of research such as computer vision.

With wav2vec~2.0, \citet{baevskiWav2vec20Framework} use unlabeled speech data to train an encoder-only model to learn contextualized representations of jointly learnt quantized speech units.
Later on, they add a linear decoder and fine-tune their model on labeled data using a Connectionist Temporal Classificiation (CTC) loss \cite{gravesConnectionistTemporalClassification} to perform tasks such as speech recognition down the line.

\subsubsection{Model Architecture}

Their model consists of a speech feature encoder, consisting of a stack of 1D convolutional layers each followed by layer normalization \cite{baLayerNormalization2016} and a GELU activation function \cite{hendrycksGaussianErrorLinear2023}.
This feature encoder builds latent feature representations for a set number of timesteps directly from the raw input waveform.

The output of the feature encoder is then passed to the so called context network, which in the pre-training phase consists only of the encoder part of a Transformer \cite{vaswaniAttentionAllYou2023}.
For encoding positional information, \citet{baevskiWav2vec20Framework} use relative positional embeddings by adding a convolution of the inputs followed by a GELU activation on to the inputs and then applying layer normalization.

\subsubsection{Self-Supervised Pre-Training}

During pre-training, random time spans in the input audio are chosen and the corresponding speech feature vectors computed by the feature encoder are masked out, similar to the self-supervised training methods used in other masked language models such as BERT \cite{devlinBERTPretrainingDeep2019}.
Masked out feature vectors are simply replaced by a trained feature vector shared between all masked time steps.

The masked feature vectors are then passed to the context network, producing contextualized representations of speech for each time step.

A loss is then calculated for each masked time step, based on a contrastive learning objective.
A quantization module computes quantized speech representations for each of the masked feature vectors by discretizing each unmasked feature to a finite set of speech representations using product quantization \cite{jegouProductQuantizationNearest2011}, an approximate nearest-neighbor method that maps each feature vector to a combination of learned codebook entries.
Each of the quantized representations are then used in a classification problem:
Given the contextualized representation for a certain masked time step, the model must identify the correct quantized speech representation associated with that time step from a set of candidate representations containing distractors.
The distractor representations for this contrastive task are quantized representation sampled from some of the other masked time steps within the same utterance, ensuring that the negative examples are acoustically and contextually plausible.
Rather than employing an explicit classification loss, the cosine similarity scores between the contextualized representation of the masked time step and each candidate quantized representation is used to calculate the contrastive loss.

This training objective encourages the similarity score of the true target to be higher than those of the distractors, thereby pushing the model to align contextual representations with the learnt discrete speech units.
Since the masked input provides no direct acoustic evidence, success on this task requires the model to learn representations that capture meaningful contextual information from the surrounding speech.

Finally, because the contrastive loss includes the quantized speech representations, gradients can propagate back through the quantization module during training.
As a result, the codebook embeddings used for quantization are learned jointly with the rest of the model, allowing the quantized speech units themselves to adapt to the structure of the speech data.

\subsubsection{Fine-Tuning}

After pre-training, the model is then fine-tuned for speech recognition.
A linear decoder is added onto the context network, projecting the learned speech representations onto a set number of classes, representing the task specific vocabulary.
This new model is then optimized using labeled speech data and minimizing CTC loss \cite{gravesConnectionistTemporalClassification}.

%TODO: should i explain how CTC loss works?

\subsubsection{Results}

\subsubsection{Connectionist Temporal Classificiation Loss}


\subsubsection{Conclusions}

% con: CTC makes strong independence assumption
% -> leads to what errors?
% bidirectional attention defeats this assumption?
% balanced out by applying LLM how?

% encoder-only models/masked language models are generally used for interpretative tasks instead of generation

% attention looks backwards AND forwards -> only applicable on offline data?

\citet{baevskiWav2vec20Framework} conclude that pre-training speech recognition models on unlabeled data shows large potential, especially for ultra-low resource scenarios with very little labeled speech data available.
This could improve the availability of speech recognition systems for many lesser spoken languages as well as dialects, as this still remains a challenge in ASR (see section \ref{sec:challenges_in_asr}).

% TODO: compare size of models

\subsection{Attention-based Encoder-Decoder Architecture}

% make it clear, that whisper improves on wav2vec 2.0, without the unsupervised techniques
Due to the unavailability of high-quality large labeled datasets for ASR in many languages, self-supervision techniques such as wav2vec 2.0 \cite{baevskiWav2vec20Framework} made use of the large amounts of readily available unlabeled speech data, achieving astounding success.
Separately, research in other fields such as computer vision showed that training on much larger weakly supervised datasets, rather than only high-quality labeled data, leads to significant improvements in robustness and generalization of models \cite{radfordRobustSpeechRecognition2023}.

% TODO: make clear, that it was a huge feat to construct these new large datasets
Motivated by these trends, \citet{radfordRobustSpeechRecognition2023} identified a key limitation of ASR models such as wav2vec~2.0, which only use a pre-trained encoder: whilst they learn powerful speech representations, they lack an equivalently strong pre-trained decoder and have to therefore rely heavily on fine-tuning to perform tasks such as speech recognition.
\citet{radfordRobustSpeechRecognition2023} instead pre-trained an entire encoder-decoder Transformer on large amounts ($680{,}000$ hours) of weakly supervised labeled speech data, successfully achieving a new state of the art with the Whisper models.
% no need for self-supervision, instead relying on more weakly-supervised data
% wav2vec 2.0 better for low-resource languages?

% whisper doesn't use (inverse) text normalization, except when filtering out bad transcripts


% supervised fine-tuning after unsupervised pre-training in wav2vec 2.0/weakly-supervised pre-training in whisper
% convolutions as preprocessing step
% pre-activation residual blocks by having layer norm applied first

% positional encoding

% in whisper:
% 1d convolutional layers compress input sequence and compute speech features

% talk about WER calculation by applying text normalization first

% general trends:
% more capable models (e.g. translation, multi-language in whisper) trained on even more (labeled) data
% unsupervised learning in (ultra-)low-resource scenarios?
% transformer has emerged as superior architecture
% more of the ASR pipeline built directly into models, such as inverse text normalization in whisper
% more joint learning e.g. of representations, positional encodings, attention

\bibliography{zotero}

\end{document}
